#!/usr/bin/env python3
"""
æŸ¥è¯¢åˆ†è§£å™¨æ¨¡å—
åŸºäºyoutu-graphragçš„å…ˆè¿›æ€æƒ³ï¼Œå®ç°æ™ºèƒ½æŸ¥è¯¢åˆ†è§£åŠŸèƒ½
æ”¯æŒå¤šè·³æ¨ç†æŸ¥è¯¢çš„è‡ªåŠ¨åˆ†è§£å’Œå¹¶è¡Œæ£€ç´¢
"""

import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import yaml
from loguru import logger

try:
    import json_repair
except ImportError:
    # å¦‚æœæ²¡æœ‰json_repairï¼Œä½¿ç”¨æ ‡å‡†json
    import json as json_repair

from agenticx.llms import LlmFactory
from agenticx.knowledge.graphers.config import LLMConfig


@dataclass
class SubQuestion:
    """å­é—®é¢˜æ•°æ®ç»“æ„"""
    question: str
    confidence: float
    reasoning_type: str  # "factual", "relational", "comparative", "temporal"
    entities: List[str]
    relations: List[str]
    priority: int = 1  # 1-é«˜ä¼˜å…ˆçº§, 2-ä¸­ä¼˜å…ˆçº§, 3-ä½ä¼˜å…ˆçº§


@dataclass
class DecompositionResult:
    """æŸ¥è¯¢åˆ†è§£ç»“æœ"""
    original_query: str
    sub_questions: List[SubQuestion]
    decomposition_confidence: float
    reasoning_complexity: str  # "simple", "medium", "complex"
    involved_types: Dict[str, List[str]]
    decomposition_strategy: str
    should_decompose: bool


class QueryDecomposer:
    """
    æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨
    åŸºäºyoutu-graphragçš„agentic_decomposeræ€æƒ³ï¼Œç»“åˆAgenticXæ¡†æ¶
    """
    
    def __init__(self, config: Dict[str, Any], schema_path: Optional[str] = None):
        """åˆå§‹åŒ–æŸ¥è¯¢åˆ†è§£å™¨"""
        self.config = config
        self.decomposition_config = config.get('retrieval', {}).get('query_decomposition', {})
        self.schema_path = schema_path or "schema.json"
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        llm_config = config.get('llm', {})
        decomposition_model = self.decomposition_config.get('decomposition_model', 'qwen3-max')
        
        # ä½¿ç”¨å¼ºæ¨¡å‹è¿›è¡ŒæŸ¥è¯¢åˆ†è§£
        strong_model_config = config.get('llm', {}).get('strong_model', llm_config)
        
        # åˆ›å»ºLLMConfigå¯¹è±¡
        llm_config_obj = LLMConfig(
            type=strong_model_config.get('provider', 'bailian'),
            model=decomposition_model,
            api_key=strong_model_config.get('api_key'),
            base_url=strong_model_config.get('base_url'),
            timeout=strong_model_config.get('timeout'),
            max_retries=strong_model_config.get('max_retries'),
            temperature=strong_model_config.get('temperature'),
            max_tokens=strong_model_config.get('max_tokens')
        )
        
        self.llm_client = LlmFactory.create_llm(llm_config_obj)
        
        # åˆ†è§£é…ç½®å‚æ•°
        self.enable_decomposition = self.decomposition_config.get('enable_decomposition', True)
        self.decomposition_count = self.decomposition_config.get('decomposition_count', 3)
        self.min_query_length = self.decomposition_config.get('min_query_length', 10)
        self.confidence_threshold = self.decomposition_config.get('confidence_threshold', 0.7)
        self.enable_schema_aware = self.decomposition_config.get('enable_schema_aware', True)
        self.enable_entity_focus = self.decomposition_config.get('enable_entity_focus', True)
        self.fallback_to_original = self.decomposition_config.get('fallback_to_original', True)
        
        # åŠ è½½å›¾æ¨¡å¼
        self.schema = self._load_schema()
        
        logger.info(f"æŸ¥è¯¢åˆ†è§£å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {decomposition_model}")

    def _load_schema(self) -> str:
        """åŠ è½½å›¾æ¨¡å¼"""
        try:
            if Path(self.schema_path).exists():
                with open(self.schema_path, 'r', encoding='utf-8') as f:
                    schema_data = json.load(f)
                    return json.dumps(schema_data, ensure_ascii=False, indent=2)
            else:
                logger.warning(f"Schemaæ–‡ä»¶ä¸å­˜åœ¨: {self.schema_path}")
                return "{}"
        except Exception as e:
            logger.error(f"åŠ è½½Schemaå¤±è´¥: {e}")
            return "{}"

    def _analyze_query_complexity(self, query: str) -> Tuple[str, float]:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        # ç®€å•å¯å‘å¼è§„åˆ™
        complexity_indicators = {
            'simple': ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'ä»‹ç»'],
            'medium': ['å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'åŸå› ', 'æ–¹æ³•', 'è¿‡ç¨‹'],
            'complex': ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å“ªä¸ªæ›´', 'æœ€', 'å…³ç³»', 'å½±å“', 'å¯¼è‡´']
        }
        
        query_lower = query.lower()
        
        # æ£€æŸ¥å¤æ‚åº¦æŒ‡æ ‡
        for complexity, indicators in complexity_indicators.items():
            if any(indicator in query_lower for indicator in indicators):
                confidence = 0.8 if complexity == 'complex' else 0.6
                return complexity, confidence
        
        # åŸºäºæŸ¥è¯¢é•¿åº¦åˆ¤æ–­
        if len(query) > 50:
            return 'complex', 0.7
        elif len(query) > 20:
            return 'medium', 0.6
        else:
            return 'simple', 0.5

    def _should_decompose(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è§£æŸ¥è¯¢"""
        if not self.enable_decomposition:
            return False
            
        if len(query) < self.min_query_length:
            return False
            
        complexity, confidence = self._analyze_query_complexity(query)
        
        # å¤æ‚æŸ¥è¯¢éœ€è¦åˆ†è§£
        if complexity in ['medium', 'complex'] and confidence > 0.6:
            return True
            
        # åŒ…å«å¤šä¸ªå®ä½“æˆ–å…³ç³»çš„æŸ¥è¯¢
        entity_keywords = ['å’Œ', 'ä¸', 'ä»¥åŠ', 'è¿˜æœ‰', 'å¯¹æ¯”', 'æ¯”è¾ƒ']
        if any(keyword in query for keyword in entity_keywords):
            return True
            
        return False

    def should_decompose(self, query: str) -> bool:
        """å…¬å…±æ–¹æ³•ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è§£æŸ¥è¯¢"""
        return self._should_decompose(query)

    def _create_decomposition_prompt(self, query: str, language: str = "chinese") -> str:
        """åˆ›å»ºåˆ†è§£æç¤ºè¯"""
        if language == "chinese":
            return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜åˆ†è§£å¤§å¸ˆï¼Œæ“…é•¿å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºç®€å•çš„å­é—®é¢˜ã€‚
è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜å’Œå›¾æœ¬ä½“æ¨¡å¼ï¼Œå°†é—®é¢˜åˆ†è§£ä¸º{self.decomposition_count}ä¸ªå­é—®é¢˜ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. æ¯ä¸ªå­é—®é¢˜å¿…é¡»ï¼š
   - æ˜ç¡®ä¸”ä¸“æ³¨äºä¸€ä¸ªäº‹å®æˆ–å…³ç³»
   - èƒ½å¤Ÿç‹¬ç«‹å›ç­”ï¼Œä¸ä¾èµ–å…¶ä»–å­é—®é¢˜çš„ç»“æœ
   - æ˜ç¡®å¼•ç”¨åŸå§‹é—®é¢˜ä¸­çš„å®ä½“å’Œå…³ç³»
   - è®¾è®¡ä¸ºæ£€ç´¢æœ€ç»ˆç­”æ¡ˆæ‰€éœ€çš„ç›¸å…³çŸ¥è¯†

2. åˆ†è§£ç­–ç•¥ï¼š
   - å¯¹äºç®€å•é—®é¢˜ï¼ˆ1-2è·³æ¨ç†ï¼‰ï¼Œè¿”å›åŸå§‹é—®é¢˜ä½œä¸ºå•ä¸ªå­é—®é¢˜
   - å¯¹äºå¤æ‚é—®é¢˜ï¼ŒæŒ‰ç…§é€»è¾‘æ¨ç†é“¾åˆ†è§£
   - ä¼˜å…ˆåˆ†è§£å®ä½“è¯†åˆ«ã€å…³ç³»æŸ¥è¯¢ã€å±æ€§è·å–ç­‰åŸºç¡€é—®é¢˜

3. è¿”å›æ ¼å¼ï¼š
   è¯·è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
   - sub_questions: å­é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«questionã€confidenceã€reasoning_typeã€entitiesã€relations
   - decomposition_confidence: åˆ†è§£ç½®ä¿¡åº¦(0-1)
   - reasoning_complexity: æ¨ç†å¤æ‚åº¦("simple", "medium", "complex")
   - involved_types: æ¶‰åŠçš„èŠ‚ç‚¹ç±»å‹ã€å…³ç³»ç±»å‹ã€å±æ€§ç±»å‹

åŸå§‹é—®é¢˜ï¼š{query}

å›¾æœ¬ä½“æ¨¡å¼ï¼š
{self.schema}

ç¤ºä¾‹è¾“å‡ºï¼š
{{
    "sub_questions": [
        {{
            "question": "ä»€ä¹ˆæ˜¯æ™ºå–ç”Ÿè¾°çº²äº‹ä»¶ï¼Ÿ",
            "confidence": 0.9,
            "reasoning_type": "factual",
            "entities": ["æ™ºå–ç”Ÿè¾°çº²"],
            "relations": ["å®šä¹‰", "æè¿°"]
        }},
        {{
            "question": "æ™ºå–ç”Ÿè¾°çº²äº‹ä»¶ä¸­çš„ä¸»è¦äººç‰©æœ‰å“ªäº›ï¼Ÿ",
            "confidence": 0.8,
            "reasoning_type": "relational",
            "entities": ["æ™ºå–ç”Ÿè¾°çº²", "äººç‰©"],
            "relations": ["å‚ä¸", "æ¶‰åŠ"]
        }}
    ],
    "decomposition_confidence": 0.85,
    "reasoning_complexity": "medium",
    "involved_types": {{
        "nodes": ["EVENT", "PERSON"],
        "relations": ["PARTICIPATES_IN", "INVOLVES"],
        "attributes": ["name", "role"]
    }}
}}
"""
        else:
            return f"""
You are a professional question decomposition expert specializing in multi-hop reasoning.
Given the following schema and question, decompose the complex question into {self.decomposition_count} focused sub-questions.

CRITICAL REQUIREMENTS:
1. Each sub-question must be:
   - Specific and focused on a single fact or relationship
   - Answerable independently with the given schema
   - Explicitly reference entities and relations from the original question
   - Designed to retrieve relevant knowledge for the final answer

2. Decomposition Strategy:
   - For simple questions (1-2 hop), return the original question as a single sub-question
   - For complex questions, decompose along logical reasoning chains
   - Prioritize entity identification, relationship queries, and attribute retrieval

3. Return Format:
   Return a JSON object with the following fields:
   - sub_questions: List of sub-questions with question, confidence, reasoning_type, entities, relations
   - decomposition_confidence: Confidence in decomposition (0-1)
   - reasoning_complexity: Reasoning complexity ("simple", "medium", "complex")
   - involved_types: Involved node types, relation types, attribute types

Original Question: {query}

Graph Schema:
{self.schema}

Example Output:
{{
    "sub_questions": [
        {{
            "question": "Who is the director of Ethnic Notions?",
            "confidence": 0.9,
            "reasoning_type": "factual",
            "entities": ["Ethnic Notions"],
            "relations": ["directed_by"]
        }},
        {{
            "question": "When did the director of Ethnic Notions die?",
            "confidence": 0.8,
            "reasoning_type": "temporal",
            "entities": ["director", "Ethnic Notions"],
            "relations": ["death_date", "directed_by"]
        }}
    ],
    "decomposition_confidence": 0.85,
    "reasoning_complexity": "medium",
    "involved_types": {{
        "nodes": ["FILM", "PERSON"],
        "relations": ["DIRECTED_BY", "DEATH_DATE"],
        "attributes": ["name", "date"]
    }}
}}
"""

    async def decompose_query(self, query: str) -> DecompositionResult:
        """åˆ†è§£æŸ¥è¯¢ä¸ºå­é—®é¢˜"""
        logger.info(f"å¼€å§‹åˆ†è§£æŸ¥è¯¢: {query}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†è§£
        should_decompose = self._should_decompose(query)
        if not should_decompose:
            logger.info("æŸ¥è¯¢æ— éœ€åˆ†è§£ï¼Œè¿”å›åŸæŸ¥è¯¢")
            return DecompositionResult(
                original_query=query,
                sub_questions=[SubQuestion(
                    question=query,
                    confidence=1.0,
                    reasoning_type="simple",
                    entities=[],
                    relations=[]
                )],
                decomposition_confidence=1.0,
                reasoning_complexity="simple",
                involved_types={"nodes": [], "relations": [], "attributes": []},
                decomposition_strategy="no_decomposition",
                should_decompose=False
            )
        
        try:
            # æ£€æµ‹è¯­è¨€
            language = "chinese" if any('\u4e00' <= char <= '\u9fff' for char in query) else "english"
            
            # åˆ›å»ºåˆ†è§£æç¤ºè¯
            prompt = self._create_decomposition_prompt(query, language)
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†è§£
            response = await self.llm_client.ainvoke(prompt)
            
            # è§£æå“åº”
            try:
                # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤markdownä»£ç å—æ ‡è®°
                raw_content = response.content.strip()
                
                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                if raw_content.startswith('```json'):
                    raw_content = raw_content[7:]  # ç§»é™¤ ```json
                if raw_content.startswith('```'):
                    raw_content = raw_content[3:]   # ç§»é™¤ ```
                if raw_content.endswith('```'):
                    raw_content = raw_content[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
                
                raw_content = raw_content.strip()
                logger.debug(f"æ¸…ç†åçš„JSONå†…å®¹: {raw_content[:200]}...")
                
                content = json_repair.loads(raw_content)
                logger.info(f"âœ… JSONè§£ææˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                logger.error(f"åŸå§‹å“åº”: {response.content}")
                logger.error(f"æ¸…ç†åå†…å®¹: {raw_content if 'raw_content' in locals() else 'N/A'}")
                # å›é€€åˆ°åŸæŸ¥è¯¢
                if self.fallback_to_original:
                    return self._create_fallback_result(query)
                raise
            
            # æ„å»ºå­é—®é¢˜åˆ—è¡¨
            sub_questions = []
            for i, sub_q in enumerate(content.get('sub_questions', [])):
                sub_question = SubQuestion(
                    question=sub_q.get('question', ''),
                    confidence=sub_q.get('confidence', 0.5),
                    reasoning_type=sub_q.get('reasoning_type', 'factual'),
                    entities=sub_q.get('entities', []),
                    relations=sub_q.get('relations', []),
                    priority=i + 1
                )
                sub_questions.append(sub_question)
            
            # æ„å»ºåˆ†è§£ç»“æœ
            result = DecompositionResult(
                original_query=query,
                sub_questions=sub_questions,
                decomposition_confidence=content.get('decomposition_confidence', 0.7),
                reasoning_complexity=content.get('reasoning_complexity', 'medium'),
                involved_types=content.get('involved_types', {}),
                decomposition_strategy="llm_decomposition",
                should_decompose=True
            )
            
            logger.info(f"æŸ¥è¯¢åˆ†è§£å®Œæˆï¼Œç”Ÿæˆ{len(sub_questions)}ä¸ªå­é—®é¢˜")
            return result
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢åˆ†è§£å¤±è´¥: {e}")
            if self.fallback_to_original:
                return self._create_fallback_result(query)
            raise

    def _create_fallback_result(self, query: str) -> DecompositionResult:
        """åˆ›å»ºå›é€€ç»“æœ"""
        return DecompositionResult(
            original_query=query,
            sub_questions=[SubQuestion(
                question=query,
                confidence=1.0,
                reasoning_type="fallback",
                entities=[],
                relations=[]
            )],
            decomposition_confidence=0.5,
            reasoning_complexity="unknown",
            involved_types={"nodes": [], "relations": [], "attributes": []},
            decomposition_strategy="fallback",
            should_decompose=False
        )

    async def parallel_decompose_and_retrieve(self, query: str, retriever) -> Tuple[List[Any], Dict[str, Any]]:
        """å¹¶è¡Œåˆ†è§£å’Œæ£€ç´¢"""
        # 1. åˆ†è§£æŸ¥è¯¢
        decomposition_result = await self.decompose_query(query)
        
        if not decomposition_result.should_decompose:
            # ä¸éœ€è¦åˆ†è§£ï¼Œç›´æ¥æ£€ç´¢
            results = await retriever.retrieve_single_query(query)
            return results, {
                'decomposition_result': decomposition_result,
                'retrieval_strategy': 'direct',
                'sub_question_count': 1
            }
        
        # 2. å¹¶è¡Œæ£€ç´¢å­é—®é¢˜
        parallel_retrieval = self.decomposition_config.get('parallel_retrieval', True)
        
        # è®°å½•å­é—®é¢˜è¯¦æƒ…
        logger.info(f"ğŸ“‹ å¼€å§‹æ£€ç´¢{len(decomposition_result.sub_questions)}ä¸ªå­é—®é¢˜:")
        for i, sub_q in enumerate(decomposition_result.sub_questions, 1):
            logger.info(f"  {i}. [{sub_q.reasoning_type}] {sub_q.question} (ç½®ä¿¡åº¦: {sub_q.confidence:.2f})")
        
        if parallel_retrieval:
            # å¹¶è¡Œæ£€ç´¢
            logger.info("ğŸ”„ æ‰§è¡Œå¹¶è¡Œæ£€ç´¢...")
            tasks = []
            for sub_q in decomposition_result.sub_questions:
                task = retriever.retrieve_single_query(sub_q.question)
                tasks.append(task)
            
            sub_results = await asyncio.gather(*tasks, return_exceptions=True)
        else:
            # é¡ºåºæ£€ç´¢
            logger.info("ğŸ”„ æ‰§è¡Œé¡ºåºæ£€ç´¢...")
            sub_results = []
            for i, sub_q in enumerate(decomposition_result.sub_questions, 1):
                try:
                    logger.info(f"  æ£€ç´¢å­é—®é¢˜ {i}/{len(decomposition_result.sub_questions)}: {sub_q.question[:50]}...")
                    result = await retriever.retrieve_single_query(sub_q.question)
                    sub_results.append(result)
                    logger.info(f"  âœ… å­é—®é¢˜ {i} æ£€ç´¢å®Œæˆ: {len(result)}ä¸ªç»“æœ")
                except Exception as e:
                    logger.error(f"  âŒ å­é—®é¢˜ {i} æ£€ç´¢å¤±è´¥: {e}")
                    sub_results.append([])
        
        # è®°å½•æ¯ä¸ªå­é—®é¢˜çš„æ£€ç´¢ç»Ÿè®¡
        logger.info("ğŸ“Š å­é—®é¢˜æ£€ç´¢ç»Ÿè®¡:")
        total_results = 0
        for i, (sub_q, result) in enumerate(zip(decomposition_result.sub_questions, sub_results), 1):
            if isinstance(result, Exception):
                logger.warning(f"  {i}. âŒ å¼‚å¸¸: {result}")
                result_count = 0
            else:
                result_count = len(result) if isinstance(result, list) else 0
                total_results += result_count
                status = "âœ…" if result_count > 0 else "âš ï¸"
                logger.info(f"  {i}. {status} [{sub_q.reasoning_type}] {result_count}ä¸ªç»“æœ - {sub_q.question[:60]}...")
        
        logger.info(f"ğŸ“ˆ æ£€ç´¢æ±‡æ€»: æ€»è®¡{total_results}ä¸ªç»“æœæ¥è‡ª{len(decomposition_result.sub_questions)}ä¸ªå­é—®é¢˜")
        
        # 3. åˆå¹¶ç»“æœ
        logger.info("ğŸ”€ å¼€å§‹åˆå¹¶å­é—®é¢˜ç»“æœ...")
        merged_results = self._merge_sub_results(
            sub_results, 
            decomposition_result.sub_questions
        )
        logger.info(f"âœ¨ åˆå¹¶å®Œæˆ: æœ€ç»ˆè·å¾—{len(merged_results)}ä¸ªå»é‡åçš„ç»“æœ")
        
        return merged_results, {
            'decomposition_result': decomposition_result,
            'retrieval_strategy': 'parallel' if parallel_retrieval else 'sequential',
            'sub_question_count': len(decomposition_result.sub_questions),
            'sub_results_count': [len(r) if isinstance(r, list) else 0 for r in sub_results]
        }

    def _merge_sub_results(self, sub_results: List[List[Any]], sub_questions: List[SubQuestion]) -> List[Any]:
        """åˆå¹¶å­é—®é¢˜çš„æ£€ç´¢ç»“æœ"""
        merge_strategy = self.decomposition_config.get('merge_strategy', 'weighted_score')
        
        all_results = []
        seen_contents = set()
        
        for i, (results, sub_q) in enumerate(zip(sub_results, sub_questions)):
            if isinstance(results, Exception):
                logger.error(f"å­é—®é¢˜æ£€ç´¢å¼‚å¸¸: {results}")
                continue
                
            if not isinstance(results, list):
                continue
                
            for result in results:
                # å»é‡
                content_key = getattr(result, 'content', str(result))[:100]
                if content_key in seen_contents:
                    continue
                seen_contents.add(content_key)
                
                # æ ¹æ®åˆå¹¶ç­–ç•¥è°ƒæ•´åˆ†æ•°
                if merge_strategy == 'weighted_score':
                    # æ ¹æ®å­é—®é¢˜ç½®ä¿¡åº¦å’Œä¼˜å…ˆçº§è°ƒæ•´åˆ†æ•°
                    weight = sub_q.confidence * (1.0 / sub_q.priority)
                    if hasattr(result, 'score'):
                        result.score = result.score * weight
                elif merge_strategy == 'relevance_ranking':
                    # ä¿æŒåŸå§‹åˆ†æ•°ï¼Œåç»­é‡æ–°æ’åº
                    pass
                
                # æ·»åŠ åˆ†è§£ä¿¡æ¯åˆ°å…ƒæ•°æ®
                if hasattr(result, 'metadata'):
                    result.metadata.update({
                        'sub_question': sub_q.question,
                        'sub_question_confidence': sub_q.confidence,
                        'reasoning_type': sub_q.reasoning_type,
                        'decomposition_priority': sub_q.priority
                    })
                
                all_results.append(result)
        
        # æ’åºå’Œé™åˆ¶ç»“æœæ•°é‡
        if merge_strategy == 'weighted_score':
            all_results.sort(key=lambda x: getattr(x, 'score', 0), reverse=True)
        elif merge_strategy == 'relevance_ranking':
            # å¯ä»¥åœ¨è¿™é‡Œå®ç°æ›´å¤æ‚çš„ç›¸å…³æ€§æ’åº
            all_results.sort(key=lambda x: getattr(x, 'score', 0), reverse=True)
        
        # é™åˆ¶ç»“æœæ•°é‡
        max_results = self.decomposition_config.get('max_merged_results', 100)
        return all_results[:max_results]

    def get_decomposition_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†è§£ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'config': self.decomposition_config,
            'schema_loaded': bool(self.schema and self.schema != "{}"),
            'llm_model': self.decomposition_config.get('decomposition_model', 'unknown')
        }


# å·¥å‚å‡½æ•°
def create_query_decomposer(config: Dict[str, Any], schema_path: Optional[str] = None) -> QueryDecomposer:
    """åˆ›å»ºæŸ¥è¯¢åˆ†è§£å™¨å®ä¾‹"""
    return QueryDecomposer(config, schema_path)