#!/usr/bin/env python3
"""
æ™ºèƒ½æŸ¥è¯¢é¢„å¤„ç†å™¨
è§£å†³ä¸­æ–‡åˆ†è¯ã€æŸ¥è¯¢æ‰©å±•å’Œå®ä½“è¯†åˆ«é—®é¢˜
"""

import re
import jieba
from typing import List, Dict, Any, Set, Optional
from dataclasses import dataclass
from loguru import logger


@dataclass
class ProcessedQuery:
    """å¤„ç†åçš„æŸ¥è¯¢ç»“æœ"""
    original: str
    normalized: str
    keywords: List[str]
    entities: List[str]
    expanded_terms: List[str]
    query_type: str
    confidence: float


class ChineseQueryProcessor:
    """ä¸­æ–‡æŸ¥è¯¢é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        # åˆå§‹åŒ–jiebaåˆ†è¯
        jieba.initialize()
        
        # å¸¸è§çš„æŸ¥è¯¢æ¨¡å¼
        self.question_patterns = {
            r'(.+?)æ˜¯ä»€ä¹ˆ': 'definition',
            r'(.+?)æ˜¯å•¥': 'definition', 
            r'ä»€ä¹ˆæ˜¯(.+?)': 'definition',
            r'(.+?)æ€ä¹ˆæ ·': 'evaluation',
            r'(.+?)å¦‚ä½•': 'method',
            r'(.+?)çš„ä½œç”¨': 'function',
            r'(.+?)çš„ç‰¹ç‚¹': 'feature',
        }
        
        # åŒä¹‰è¯è¯å…¸
        self.synonyms = {
            'æ˜¯å•¥': ['æ˜¯ä»€ä¹ˆ', 'æ˜¯', 'å®šä¹‰', 'å«ä¹‰'],
            'æ€ä¹ˆæ ·': ['å¦‚ä½•', 'æ€æ ·', 'æ•ˆæœ'],
            'ä½œç”¨': ['åŠŸèƒ½', 'ç”¨é€”', 'ç›®çš„'],
            'ç‰¹ç‚¹': ['ç‰¹å¾', 'æ€§è´¨', 'å±æ€§'],
        }
        
        # å®ä½“ç±»å‹è¯å…¸
        self.entity_types = {
            'å…¬å¸': ['å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'æœ‰é™å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸'],
            'æŠ€æœ¯': ['æŠ€æœ¯', 'ç®—æ³•', 'æ–¹æ³•', 'æ¡†æ¶', 'ç³»ç»Ÿ'],
            'äº§å“': ['äº§å“', 'æœåŠ¡', 'å¹³å°', 'å·¥å…·'],
            'æ¦‚å¿µ': ['æ¦‚å¿µ', 'ç†è®º', 'æ¨¡å‹', 'åŸç†'],
        }
        
        # åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
        }

    def process_query(self, query: str) -> ProcessedQuery:
        """å¤„ç†æŸ¥è¯¢"""
        try:
            # 1. æ ‡å‡†åŒ–æŸ¥è¯¢
            normalized = self._normalize_query(query)
            
            # 2. è¯†åˆ«æŸ¥è¯¢ç±»å‹
            query_type, confidence = self._identify_query_type(normalized)
            
            # 3. æå–å…³é”®è¯
            keywords = self._extract_keywords(normalized)
            
            # 4. å®ä½“è¯†åˆ«
            entities = self._extract_entities(normalized)
            
            # 5. æŸ¥è¯¢æ‰©å±•
            expanded_terms = self._expand_query(keywords, entities)
            
            return ProcessedQuery(
                original=query,
                normalized=normalized,
                keywords=keywords,
                entities=entities,
                expanded_terms=expanded_terms,
                query_type=query_type,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€å¤„ç†ç»“æœ
            return ProcessedQuery(
                original=query,
                normalized=query.strip(),
                keywords=[query.strip()],
                entities=[],
                expanded_terms=[query.strip()],
                query_type='unknown',
                confidence=0.5
            )

    def _normalize_query(self, query: str) -> str:
        """æ ‡å‡†åŒ–æŸ¥è¯¢"""
        # å»é™¤å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', query.strip())
        
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        normalized = normalized.replace('ï¼Ÿ', '?').replace('ï¼', '!')
        
        # å¤„ç†å¸¸è§çš„å£è¯­åŒ–è¡¨è¾¾
        replacements = {
            'æ˜¯å•¥': 'æ˜¯ä»€ä¹ˆ',
            'å’‹æ ·': 'æ€ä¹ˆæ ·',
            'å’‹åŠ': 'æ€ä¹ˆåŠ',
            'å•¥æ„æ€': 'ä»€ä¹ˆæ„æ€',
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized

    def _identify_query_type(self, query: str) -> tuple[str, float]:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ é—®å€™è¯­æ£€æµ‹
        greetings = ['ä½ å¥½', 'hello', 'hi', 'æ‚¨å¥½', 'æ—©ä¸Šå¥½', 'ä¸‹åˆå¥½', 'æ™šä¸Šå¥½', 'nihao']
        if any(greeting in query.lower() for greeting in greetings):
            return 'greeting', 0.95
        
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ— æ„ä¹‰æŸ¥è¯¢æ£€æµ‹
        meaningless = ['æµ‹è¯•', 'test', 'è¯•è¯•', 'çœ‹çœ‹', 'éšä¾¿', 'æ²¡äº‹']
        if any(word in query.lower() for word in meaningless) and len(query) < 10:
            return 'meaningless', 0.9
        
        for pattern, query_type in self.question_patterns.items():
            if re.search(pattern, query):
                return query_type, 0.9
        
        # åŸºäºå…³é”®è¯åˆ¤æ–­
        if any(word in query for word in ['ä»€ä¹ˆ', 'æ˜¯', 'å®šä¹‰']):
            return 'definition', 0.7
        elif any(word in query for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ–¹æ³•']):
            return 'method', 0.7
        elif any(word in query for word in ['ä¸ºä»€ä¹ˆ', 'åŸå› ']):
            return 'reason', 0.7
        else:
            return 'general', 0.5

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ä½¿ç”¨jiebaåˆ†è¯
        words = list(jieba.cut(query))
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = []
        for word in words:
            word = word.strip()
            if (len(word) > 1 and 
                word not in self.stop_words and 
                not re.match(r'^[ï¼Ÿï¼ã€‚ï¼Œã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\s]+$', word)):
                keywords.append(word)
        
        return keywords

    def _extract_entities(self, query: str) -> List[str]:
        """æå–å®ä½“"""
        entities = []
        
        # åŸºäºæ¨¡å¼åŒ¹é…æå–å®ä½“
        patterns = [
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',  # è‹±æ–‡å®ä½“
            r'([\u4e00-\u9fff]{2,}(?:å…¬å¸|ä¼ä¸š|é›†å›¢|æŠ€æœ¯|ç³»ç»Ÿ|å¹³å°))',  # ä¸­æ–‡æœºæ„/æŠ€æœ¯å®ä½“
            r'([\u4e00-\u9fff]{2,})',  # ä¸€èˆ¬ä¸­æ–‡å®ä½“
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, query)
            entities.extend(matches)
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_entities = []
        for entity in entities:
            if entity not in unique_entities and len(entity) > 1:
                unique_entities.append(entity)
        
        return unique_entities

    def _expand_query(self, keywords: List[str], entities: List[str]) -> List[str]:
        """æ‰©å±•æŸ¥è¯¢è¯æ±‡"""
        expanded = set(keywords + entities)
        
        # æ·»åŠ åŒä¹‰è¯
        for keyword in keywords:
            if keyword in self.synonyms:
                expanded.update(self.synonyms[keyword])
        
        # æ·»åŠ ç›¸å…³è¯æ±‡
        for entity in entities:
            # å¦‚æœæ˜¯å…¬å¸åï¼Œæ·»åŠ ç›¸å…³è¯æ±‡
            if any(suffix in entity for suffix in ['å…¬å¸', 'ä¼ä¸š', 'é›†å›¢']):
                expanded.update(['ä¸šåŠ¡', 'æœåŠ¡', 'äº§å“'])
            # å¦‚æœæ˜¯æŠ€æœ¯åï¼Œæ·»åŠ ç›¸å…³è¯æ±‡
            elif any(suffix in entity for suffix in ['æŠ€æœ¯', 'ç³»ç»Ÿ', 'å¹³å°']):
                expanded.update(['åº”ç”¨', 'åŠŸèƒ½', 'ç‰¹ç‚¹'])
        
        return list(expanded)

    def generate_search_queries(self, processed_query: ProcessedQuery) -> List[str]:
        """ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢"""
        queries = []
        
        # 1. åŸå§‹æŸ¥è¯¢
        queries.append(processed_query.original)
        
        # 2. æ ‡å‡†åŒ–æŸ¥è¯¢
        if processed_query.normalized != processed_query.original:
            queries.append(processed_query.normalized)
        
        # 3. å…³é”®è¯ç»„åˆ
        if len(processed_query.keywords) > 1:
            queries.append(' '.join(processed_query.keywords))
        
        # 4. å®ä½“æŸ¥è¯¢
        for entity in processed_query.entities:
            if len(entity) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„å®ä½“
                queries.append(entity)
        
        # 5. æ‰©å±•è¯æŸ¥è¯¢
        if processed_query.expanded_terms:
            # é€‰æ‹©æœ€é‡è¦çš„æ‰©å±•è¯
            important_terms = [term for term in processed_query.expanded_terms 
                             if len(term) > 2 and term not in processed_query.keywords]
            if important_terms:
                queries.append(' '.join(important_terms[:3]))  # æœ€å¤š3ä¸ªæ‰©å±•è¯
        
        # å»é‡
        unique_queries = []
        for query in queries:
            if query not in unique_queries:
                unique_queries.append(query)
        
        return unique_queries

    def should_use_fuzzy_search(self, processed_query: ProcessedQuery) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æ¨¡ç³Šæœç´¢"""
        # å¦‚æœæŸ¥è¯¢å¾ˆçŸ­æˆ–è€…ç½®ä¿¡åº¦å¾ˆä½ï¼Œå»ºè®®ä½¿ç”¨æ¨¡ç³Šæœç´¢
        return (len(processed_query.original) < 5 or 
                processed_query.confidence < 0.6 or
                len(processed_query.keywords) < 2)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    processor = ChineseQueryProcessor()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "é“å¡”æ˜¯å•¥",
        "ä¸­å›½é“å¡”å…¬å¸",
        "nihao",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "AgenticXæ¡†æ¶æ€ä¹ˆæ ·"
    ]
    
    for query in test_queries:
        print(f"\nåŸå§‹æŸ¥è¯¢: {query}")
        result = processor.process_query(query)
        print(f"æ ‡å‡†åŒ–: {result.normalized}")
        print(f"å…³é”®è¯: {result.keywords}")
        print(f"å®ä½“: {result.entities}")
        print(f"æ‰©å±•è¯: {result.expanded_terms}")
        print(f"æŸ¥è¯¢ç±»å‹: {result.query_type} (ç½®ä¿¡åº¦: {result.confidence})")
        
        search_queries = processor.generate_search_queries(result)
        print(f"æœç´¢æŸ¥è¯¢: {search_queries}")
        print(f"å»ºè®®æ¨¡ç³Šæœç´¢: {processor.should_use_fuzzy_search(result)}")