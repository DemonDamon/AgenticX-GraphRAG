#!/usr/bin/env python3
"""
å¢å¼ºæ£€ç´¢å™¨
å®ç°å¤šçº§å›é€€ç­–ç•¥ã€åŠ¨æ€é˜ˆå€¼è°ƒæ•´å’Œæ™ºèƒ½æŸ¥è¯¢å¤„ç†
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from loguru import logger

from query_processor import ChineseQueryProcessor, ProcessedQuery

# ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨AgenticXæ¡†æ¶çš„æ ‡å‡†RetrievalResult
try:
    from agenticx.retrieval.base import RetrievalResult
    logger.info("ä½¿ç”¨AgenticXæ ‡å‡†RetrievalResult")
except ImportError:
    # å›é€€åˆ°è‡ªå®šä¹‰å®šä¹‰
    @dataclass
    class RetrievalResult:
        """æ£€ç´¢ç»“æœ - å›é€€å®šä¹‰"""
        content: str
        score: float
        metadata: Dict[str, Any]
        source: Optional[str] = None
        chunk_id: Optional[str] = None
        bm25_score: Optional[float] = None
    logger.warning("ä½¿ç”¨è‡ªå®šä¹‰RetrievalResultå®šä¹‰")


@dataclass
class RetrievalStrategy:
    """æ£€ç´¢ç­–ç•¥é…ç½®"""
    name: str
    vector_threshold: float
    graph_threshold: float
    bm25_min_score: float
    top_k: int
    description: str


class EnhancedRetriever:
    """å¢å¼ºæ£€ç´¢å™¨ - æ”¯æŒå¤šçº§å›é€€å’Œæ™ºèƒ½æŸ¥è¯¢å¤„ç†"""
    
    def __init__(self, base_retriever, graph_retriever=None, storage_manager=None):
        """åˆå§‹åŒ–å¢å¼ºæ£€ç´¢å™¨"""
        self.base_retriever = base_retriever
        self.graph_retriever = graph_retriever
        self.storage_manager = storage_manager
        self.query_processor = ChineseQueryProcessor()
        
        # å®šä¹‰å¤šçº§æ£€ç´¢ç­–ç•¥ - ğŸš€ ä¼˜åŒ–ï¼šæ›´æ¿€è¿›çš„é˜ˆå€¼è®¾ç½®
        self.strategies = [
            RetrievalStrategy(
                name="strict",
                vector_threshold=0.7,
                graph_threshold=0.6,
                bm25_min_score=0.5,
                top_k=20,
                description="ä¸¥æ ¼æ¨¡å¼ - é«˜è´¨é‡ç»“æœ"
            ),
            RetrievalStrategy(
                name="standard",
                vector_threshold=0.4,
                graph_threshold=0.3,
                bm25_min_score=0.2,
                top_k=50,
                description="æ ‡å‡†æ¨¡å¼ - å¹³è¡¡è´¨é‡å’Œå¬å›"
            ),
            RetrievalStrategy(
                name="relaxed",
                vector_threshold=0.2,
                graph_threshold=0.1,
                bm25_min_score=0.05,
                top_k=100,
                description="å®½æ¾æ¨¡å¼ - é«˜å¬å›ç‡"
            ),
            RetrievalStrategy(
                name="fuzzy",
                vector_threshold=0.05,
                graph_threshold=0.01,
                bm25_min_score=0.0,
                top_k=200,
                description="æ¨¡ç³Šæ¨¡å¼ - æœ€å¤§å¬å›"
            ),
            RetrievalStrategy(
                name="aggressive",
                vector_threshold=0.0,
                graph_threshold=0.0,
                bm25_min_score=0.0,
                top_k=500,
                description="æ¿€è¿›æ¨¡å¼ - æ— é˜ˆå€¼é™åˆ¶"
            )
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'strategy_usage': {s.name: 0 for s in self.strategies},
            'avg_results_per_query': 0.0
        }

    async def retrieve_with_fallback(self, query: str, **kwargs) -> Tuple[List[RetrievalResult], Dict[str, Any]]:
        """å¤šçº§å›é€€æ£€ç´¢"""
        self.stats['total_queries'] += 1
        
        try:
            # 1. æ™ºèƒ½æŸ¥è¯¢é¢„å¤„ç†
            processed_query = self.query_processor.process_query(query)
            logger.info(f"æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ: {processed_query.query_type}, ç½®ä¿¡åº¦: {processed_query.confidence}")
            
            # 2. ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢
            search_queries = self.query_processor.generate_search_queries(processed_query)
            logger.info(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢: {search_queries}")
            
            # 3. æ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©èµ·å§‹ç­–ç•¥
            start_strategy_idx = self._select_start_strategy(processed_query)
            
            # 4. å¤šçº§å›é€€æ£€ç´¢
            all_results = []
            strategy_used = None
            
            for i in range(start_strategy_idx, len(self.strategies)):
                strategy = self.strategies[i]
                logger.info(f"å°è¯•ç­–ç•¥: {strategy.name} - {strategy.description}")
                
                # å¯¹æ¯ä¸ªæœç´¢æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢
                strategy_results = []
                for search_query in search_queries:
                    results = await self._execute_strategy(search_query, strategy)
                    strategy_results.extend(results)
                
                # å»é‡å’Œæ’åº
                unique_results = self._deduplicate_results(strategy_results)
                
                if unique_results:
                    all_results = unique_results
                    strategy_used = strategy
                    self.stats['strategy_usage'][strategy.name] += 1
                    logger.info(f"ç­–ç•¥ {strategy.name} æˆåŠŸï¼Œè¿”å› {len(unique_results)} æ¡ç»“æœ")
                    break
                else:
                    logger.info(f"ç­–ç•¥ {strategy.name} æ— ç»“æœï¼Œå°è¯•ä¸‹ä¸€çº§")
            
            # 5. å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œå°è¯•å®ä½“æœç´¢
            if not all_results:
                logger.warning("æ‰€æœ‰æ£€ç´¢ç­–ç•¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å®ä½“æœç´¢")
                entity_results = await self._direct_entity_search(processed_query)
                if entity_results:
                    all_results = entity_results
                    strategy_used = RetrievalStrategy("entity_search", 0, 0, 0, 10, "ç›´æ¥å®ä½“æœç´¢")
            
            # 6. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if all_results:
                self.stats['successful_queries'] += 1
                self.stats['avg_results_per_query'] = (
                    (self.stats['avg_results_per_query'] * (self.stats['total_queries'] - 1) + len(all_results)) 
                    / self.stats['total_queries']
                )
            
            # 7. ç”Ÿæˆæ£€ç´¢æŠ¥å‘Š
            retrieval_report = {
                'original_query': query,
                'processed_query': processed_query,
                'search_queries': search_queries,
                'strategy_used': strategy_used.name if strategy_used else 'none',
                'total_results': len(all_results),
                'success': len(all_results) > 0
            }
            
            return all_results, retrieval_report
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ£€ç´¢å¤±è´¥: {e}")
            return [], {'error': str(e), 'success': False}

    def _select_start_strategy(self, processed_query: ProcessedQuery) -> int:
        """æ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©èµ·å§‹ç­–ç•¥"""
        # é«˜ç½®ä¿¡åº¦æŸ¥è¯¢ä»ä¸¥æ ¼æ¨¡å¼å¼€å§‹
        if processed_query.confidence > 0.8 and len(processed_query.entities) > 0:
            return 0  # strict
        
        # ä¸­ç­‰ç½®ä¿¡åº¦ä»æ ‡å‡†æ¨¡å¼å¼€å§‹
        elif processed_query.confidence > 0.6:
            return 1  # standard
        
        # ä½ç½®ä¿¡åº¦æˆ–çŸ­æŸ¥è¯¢ä»å®½æ¾æ¨¡å¼å¼€å§‹
        elif processed_query.confidence > 0.4 or len(processed_query.original) > 5:
            return 2  # relaxed
        
        # æä½ç½®ä¿¡åº¦ç›´æ¥ä½¿ç”¨æ¨¡ç³Šæ¨¡å¼
        else:
            return 3  # fuzzy

    async def _execute_strategy(self, query: str, strategy: RetrievalStrategy) -> List[RetrievalResult]:
        """æ‰§è¡Œç‰¹å®šç­–ç•¥çš„æ£€ç´¢"""
        try:
            results = []
            
            # 1. æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25ï¼‰
            if hasattr(self.base_retriever, 'retrieve'):
                hybrid_results = await self.base_retriever.retrieve(
                    query, 
                    top_k=strategy.top_k,
                    min_score=strategy.vector_threshold
                )
                results.extend(hybrid_results)
            
            # 2. å›¾æ£€ç´¢
            if self.graph_retriever:
                try:
                    graph_results = await self.graph_retriever.retrieve(
                        query,
                        top_k=strategy.top_k // 2,
                        min_score=strategy.graph_threshold
                    )
                    results.extend(graph_results)
                except Exception as e:
                    logger.warning(f"å›¾æ£€ç´¢å¤±è´¥: {e}")
            
            # 3. è¿‡æ»¤ä½åˆ†ç»“æœ
            filtered_results = [
                r for r in results 
                if r.score >= strategy.bm25_min_score
            ]
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"ç­–ç•¥æ‰§è¡Œå¤±è´¥: {e}")
            return []

    async def _direct_entity_search(self, processed_query: ProcessedQuery) -> List[RetrievalResult]:
        """ç›´æ¥å®ä½“æœç´¢ - ğŸš€ å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥"""
        try:
            if not self.storage_manager:
                return []
            
            from agenticx.storage import StorageType
            graph_storage = self.storage_manager.get_storage(StorageType.NEO4J)
            
            if not graph_storage:
                return []
            
            results = []
            
            # ğŸš€ ç­–ç•¥1: æœç´¢å®ä½“
            search_terms = processed_query.entities + processed_query.keywords + [processed_query.original]
            
            for term in search_terms:
                if len(term) < 1:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                    continue
                
                # ğŸš€ å¢å¼ºçš„æ¨¡ç³ŠåŒ¹é…å®ä½“æŸ¥è¯¢
                cypher_queries = [
                    # ç²¾ç¡®åŒ¹é…
                    """
                    MATCH (n:Entity) 
                    WHERE toLower(n.name) = toLower($term)
                    RETURN n.name as name, n.description as description, 
                           labels(n) as type, n.id as id, 1.0 as relevance
                    LIMIT 5
                    """,
                    # åŒ…å«åŒ¹é…
                    """
                    MATCH (n:Entity) 
                    WHERE toLower(n.name) CONTAINS toLower($term)
                       OR toLower($term) CONTAINS toLower(n.name)
                    RETURN n.name as name, n.description as description, 
                           labels(n) as type, n.id as id, 0.8 as relevance
                    LIMIT 10
                    """,
                    # æ­£åˆ™åŒ¹é…
                    """
                    MATCH (n:Entity) 
                    WHERE n.name =~ ('(?i).*' + $term + '.*')
                       OR n.description =~ ('(?i).*' + $term + '.*')
                    RETURN n.name as name, n.description as description, 
                           labels(n) as type, n.id as id, 0.6 as relevance
                    LIMIT 15
                    """,
                    # ğŸš€ æ–°å¢ï¼šæœç´¢æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬Chunkç­‰ï¼‰
                    """
                    MATCH (n) 
                    WHERE (n.name IS NOT NULL AND toLower(n.name) CONTAINS toLower($term))
                       OR (n.content IS NOT NULL AND toLower(n.content) CONTAINS toLower($term))
                       OR (n.description IS NOT NULL AND toLower(n.description) CONTAINS toLower($term))
                    RETURN n.name as name, 
                           COALESCE(n.description, n.content, '') as description, 
                           labels(n) as type, n.id as id, 0.4 as relevance
                    LIMIT 20
                    """
                ]
                
                for cypher_query in cypher_queries:
                    try:
                        entity_results = graph_storage.execute_query(
                            cypher_query, 
                            {"term": term}
                        )
                        
                        for result in entity_results:
                            if result['name']:  # ç¡®ä¿æœ‰åç§°
                                # è½¬æ¢ä¸ºRetrievalResultæ ¼å¼
                                content = f"å®ä½“: {result['name']}"
                                if result.get('description'):
                                    content += f"\næè¿°: {result['description'][:500]}..."  # é™åˆ¶æè¿°é•¿åº¦
                                
                                retrieval_result = RetrievalResult(
                                    content=content,
                                    score=result.get('relevance', 0.5),
                                    metadata={
                                        'type': 'entity',
                                        'entity_name': result['name'],
                                        'entity_type': result.get('type', []),
                                        'entity_id': result.get('id'),
                                        'search_source': 'direct_entity',
                                        'search_term': term
                                    },
                                    source='knowledge_graph'
                                )
                                results.append(retrieval_result)
                    
                    except Exception as e:
                        logger.warning(f"å®ä½“æŸ¥è¯¢å¤±è´¥: {e}")
                        continue
            
            # ğŸš€ ç­–ç•¥2: å¦‚æœå®ä½“æœç´¢æ— ç»“æœï¼Œå°è¯•å…¨æ–‡æœç´¢
            if not results:
                logger.info("å®ä½“æœç´¢æ— ç»“æœï¼Œå°è¯•å…¨æ–‡æœç´¢")
                results.extend(await self._full_text_search(processed_query, graph_storage))
            
            # ğŸš€ ç­–ç•¥3: å»é‡å’Œæ’åº
            unique_results = self._deduplicate_results(results)
            
            # ğŸš€ ç­–ç•¥4: å¦‚æœè¿˜æ˜¯æ— ç»“æœï¼Œè¿”å›ä¸€äº›é€šç”¨ä¿¡æ¯
            if not unique_results:
                logger.info("æ‰€æœ‰æœç´¢ç­–ç•¥æ— ç»“æœï¼Œè¿”å›ç³»ç»Ÿä¿¡æ¯")
                unique_results = await self._get_fallback_results(processed_query)
            
            return unique_results[:10]  # é™åˆ¶è¿”å›æ•°é‡
            
        except Exception as e:
            logger.error(f"ç›´æ¥å®ä½“æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _full_text_search(self, processed_query: ProcessedQuery, graph_storage) -> List[RetrievalResult]:
        """å…¨æ–‡æœç´¢"""
        results = []
        
        try:
            # æœç´¢æ‰€æœ‰åŒ…å«å…³é”®è¯çš„èŠ‚ç‚¹
            search_terms = [processed_query.original] + processed_query.keywords
            
            for term in search_terms:
                if len(term) < 2:
                    continue
                
                # å…¨æ–‡æœç´¢æŸ¥è¯¢
                cypher_query = """
                MATCH (n) 
                WHERE ANY(prop IN keys(n) WHERE toString(n[prop]) =~ ('(?i).*' + $term + '.*'))
                RETURN n, labels(n) as node_type, 
                       [prop IN keys(n) WHERE toString(n[prop]) =~ ('(?i).*' + $term + '.*')] as matched_props
                LIMIT 20
                """
                
                search_results = graph_storage.execute_query(cypher_query, {"term": term})
                
                for result in search_results:
                    node = result['n']
                    node_type = result['node_type']
                    matched_props = result['matched_props']
                    
                    # æ„å»ºå†…å®¹
                    content_parts = []
                    if hasattr(node, 'name') and node.name:
                        content_parts.append(f"åç§°: {node.name}")
                    if hasattr(node, 'content') and node.content:
                        content_parts.append(f"å†…å®¹: {node.content[:300]}...")
                    if hasattr(node, 'description') and node.description:
                        content_parts.append(f"æè¿°: {node.description[:200]}...")
                    
                    if content_parts:
                        retrieval_result = RetrievalResult(
                            content="\n".join(content_parts),
                            score=0.3,
                            metadata={
                                'type': 'full_text_match',
                                'node_type': node_type,
                                'matched_properties': matched_props,
                                'search_source': 'full_text',
                                'search_term': term
                            },
                            source='knowledge_graph'
                        )
                        results.append(retrieval_result)
        
        except Exception as e:
            logger.warning(f"å…¨æ–‡æœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def _get_fallback_results(self, processed_query: ProcessedQuery) -> List[RetrievalResult]:
        """è·å–å›é€€ç»“æœ"""
        fallback_results = []
        
        # æä¾›ä¸€äº›é€šç”¨çš„å¸®åŠ©ä¿¡æ¯
        fallback_content = f"""
å¾ˆæŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ "{processed_query.original}" ç›¸å…³çš„å…·ä½“ä¿¡æ¯ã€‚

ğŸ’¡ å»ºè®®æ‚¨å°è¯•ï¼š
1. ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯
2. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®
3. å°è¯•ç›¸å…³çš„åŒä¹‰è¯
4. ä½¿ç”¨æ›´ç®€å•çš„è¡¨è¾¾æ–¹å¼

ğŸ” ç³»ç»Ÿåˆ†æï¼š
- æŸ¥è¯¢ç±»å‹: {processed_query.query_type}
- è¯†åˆ«çš„å…³é”®è¯: {', '.join(processed_query.keywords) if processed_query.keywords else 'æ— '}
- è¯†åˆ«çš„å®ä½“: {', '.join(processed_query.entities) if processed_query.entities else 'æ— '}
- ç½®ä¿¡åº¦: {processed_query.confidence:.2f}
"""
        
        fallback_result = RetrievalResult(
            content=fallback_content.strip(),
            score=0.1,
            metadata={
                'type': 'fallback_help',
                'search_source': 'system_fallback',
                'original_query': processed_query.original
            },
            source='system'
        )
        
        fallback_results.append(fallback_result)
        
        return fallback_results

    def _deduplicate_results(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """å»é‡å’Œæ’åºç»“æœ"""
        if not results:
            return []
        
        # åŸºäºå†…å®¹å»é‡
        seen_content = set()
        unique_results = []
        
        for result in results:
            content_key = result.content.strip().lower()[:100]  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºå»é‡é”®
            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x.score, reverse=True)
        
        return unique_results

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (
            self.stats['successful_queries'] / self.stats['total_queries'] 
            if self.stats['total_queries'] > 0 else 0
        )
        
        return {
            'total_queries': self.stats['total_queries'],
            'successful_queries': self.stats['successful_queries'],
            'success_rate': success_rate,
            'avg_results_per_query': self.stats['avg_results_per_query'],
            'strategy_usage': self.stats['strategy_usage'],
            'most_used_strategy': max(
                self.stats['strategy_usage'].items(), 
                key=lambda x: x[1]
            )[0] if any(self.stats['strategy_usage'].values()) else 'none'
        }

    async def suggest_related_queries(self, query: str, max_suggestions: int = 5) -> List[str]:
        """åŸºäºæŸ¥è¯¢å¤±è´¥æƒ…å†µæä¾›ç›¸å…³æŸ¥è¯¢å»ºè®®"""
        try:
            processed_query = self.query_processor.process_query(query)
            suggestions = []
            
            # 1. åŸºäºå®ä½“çš„å»ºè®®
            for entity in processed_query.entities:
                suggestions.append(f"{entity}æ˜¯ä»€ä¹ˆ")
                suggestions.append(f"{entity}çš„ä½œç”¨")
                suggestions.append(f"{entity}çš„ç‰¹ç‚¹")
            
            # 2. åŸºäºå…³é”®è¯çš„å»ºè®®
            for keyword in processed_query.keywords:
                if len(keyword) > 2:
                    suggestions.append(f"{keyword}ç›¸å…³ä¿¡æ¯")
                    suggestions.append(f"å…³äº{keyword}")
            
            # 3. åŸºäºæŸ¥è¯¢ç±»å‹çš„å»ºè®®
            if processed_query.query_type == 'definition':
                suggestions.extend([
                    "ç›¸å…³æ¦‚å¿µä»‹ç»",
                    "åŸºæœ¬æ¦‚å¿µè¯´æ˜",
                    "æœ¯è¯­è§£é‡Š"
                ])
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_suggestions = []
            for suggestion in suggestions:
                if suggestion not in unique_suggestions and suggestion != query:
                    unique_suggestions.append(suggestion)
                if len(unique_suggestions) >= max_suggestions:
                    break
            
            return unique_suggestions
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŸ¥è¯¢å»ºè®®å¤±è´¥: {e}")
            return ["è¯·å°è¯•æ›´å…·ä½“çš„æŸ¥è¯¢", "ä½¿ç”¨å…³é”®è¯æœç´¢", "æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®"]


# ä½¿ç”¨ç¤ºä¾‹
async def test_enhanced_retriever():
    """æµ‹è¯•å¢å¼ºæ£€ç´¢å™¨"""
    # è¿™é‡Œéœ€è¦å®é™…çš„æ£€ç´¢å™¨å®ä¾‹
    # enhanced_retriever = EnhancedRetriever(base_retriever, graph_retriever, storage_manager)
    
    test_queries = [
        "é“å¡”æ˜¯å•¥",
        "ä¸­å›½é“å¡”å…¬å¸",
        "nihao",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯"
    ]
    
    print("å¢å¼ºæ£€ç´¢å™¨æµ‹è¯•")
    print("=" * 50)
    
    for query in test_queries:
        print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
        # results, report = await enhanced_retriever.retrieve_with_fallback(query)
        # print(f"ç»“æœæ•°é‡: {len(results)}")
        # print(f"ä½¿ç”¨ç­–ç•¥: {report.get('strategy_used', 'unknown')}")
        # print(f"æˆåŠŸ: {report.get('success', False)}")


if __name__ == "__main__":
    asyncio.run(test_enhanced_retriever())