#!/usr/bin/env python3
"""
å¢å¼ºæ£€ç´¢å™¨
å®ç°å¤šçº§å›é€€ç­–ç•¥ã€åŠ¨æ€é˜ˆå€¼è°ƒæ•´å’Œæ™ºèƒ½æŸ¥è¯¢å¤„ç†
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from loguru import logger

from query_processor import ChineseQueryProcessor, ProcessedQuery

# ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨AgenticXæ¡†æ¶çš„æ ‡å‡†RetrievalResult
try:
    from agenticx.retrieval.base import RetrievalResult
    logger.info("ä½¿ç”¨AgenticXæ ‡å‡†RetrievalResult")
except ImportError:
    # å›é€€åˆ°è‡ªå®šä¹‰å®šä¹‰
    @dataclass
    class RetrievalResult:
        """æ£€ç´¢ç»“æœ - å›é€€å®šä¹‰"""
        content: str
        score: float
        metadata: Dict[str, Any]
        source: Optional[str] = None
        chunk_id: Optional[str] = None
        bm25_score: Optional[float] = None
    logger.warning("ä½¿ç”¨è‡ªå®šä¹‰RetrievalResultå®šä¹‰")


@dataclass
class RetrievalStrategy:
    """æ£€ç´¢ç­–ç•¥é…ç½®"""
    name: str
    vector_threshold: float
    graph_threshold: float
    bm25_min_score: float
    top_k: int
    description: str


class EnhancedRetriever:
    """å¢å¼ºæ£€ç´¢å™¨ - æ”¯æŒå¤šçº§å›é€€å’Œæ™ºèƒ½æŸ¥è¯¢å¤„ç†"""
    
    def __init__(self, base_retriever, graph_retriever=None, storage_manager=None):
        """åˆå§‹åŒ–å¢å¼ºæ£€ç´¢å™¨"""
        self.base_retriever = base_retriever
        self.graph_retriever = graph_retriever
        self.storage_manager = storage_manager
        self.query_processor = ChineseQueryProcessor()
        
        # å®šä¹‰å¤šçº§æ£€ç´¢ç­–ç•¥ - ğŸš€ è°ƒæ•´ä¸ºç”¨æˆ·æœŸæœ›çš„é˜ˆå€¼ï¼šæ–‡æ¡£>0.2ï¼Œå›¾è°±>0.1
        self.strategies = [
            RetrievalStrategy(
                name="strict",
                vector_threshold=0.5,   # ğŸ”§ ä¸¥æ ¼æ¨¡å¼ä¿æŒè¾ƒé«˜é˜ˆå€¼
                graph_threshold=0.4,    # ğŸ”§ ä¸¥æ ¼å›¾æ£€ç´¢é˜ˆå€¼
                bm25_min_score=0.25,    # ğŸ”§ ä¸¥æ ¼BM25é˜ˆå€¼
                top_k=30,               # ğŸ”§ é€‚ä¸­è¿”å›æ•°é‡
                description="ä¸¥æ ¼æ¨¡å¼ - é«˜è´¨é‡ç»“æœ"
            ),
            RetrievalStrategy(
                name="standard",
                vector_threshold=0.3,   # ğŸ”§ æ ‡å‡†æ¨¡å¼é€‚ä¸­é˜ˆå€¼
                graph_threshold=0.2,    # ğŸ”§ æ ‡å‡†å›¾æ£€ç´¢é˜ˆå€¼
                bm25_min_score=0.15,    # ğŸ”§ æ ‡å‡†BM25é˜ˆå€¼
                top_k=60,               # ğŸ”§ å¢åŠ è¿”å›æ•°é‡
                description="æ ‡å‡†æ¨¡å¼ - å¹³è¡¡è´¨é‡å’Œå¬å›"
            ),
            RetrievalStrategy(
                name="relaxed",
                vector_threshold=0.2,   # ğŸ”§ ç”¨æˆ·æœŸæœ›ï¼šæ–‡æ¡£ç›¸ä¼¼åº¦>0.2
                graph_threshold=0.1,    # ğŸ”§ ç”¨æˆ·æœŸæœ›ï¼šçŸ¥è¯†å›¾è°±>0.1
                bm25_min_score=0.08,    # ğŸ”§ é€‚ä¸­çš„BM25é˜ˆå€¼
                top_k=100,              # ğŸ”§ å¢åŠ è¿”å›æ•°é‡
                description="å®½æ¾æ¨¡å¼ - é«˜å¬å›ç‡"
            ),
            RetrievalStrategy(
                name="fuzzy",
                vector_threshold=0.15,  # ğŸ”§ æ¨¡ç³Šæ¨¡å¼ç¨ä½é˜ˆå€¼
                graph_threshold=0.08,   # ğŸ”§ æ¨¡ç³Šå›¾æ£€ç´¢é˜ˆå€¼
                bm25_min_score=0.04,    # ğŸ”§ æ¨¡ç³ŠBM25åˆ†æ•°
                top_k=150,              # ğŸ”§ è¾ƒå¤šè¿”å›æ•°é‡
                description="æ¨¡ç³Šæ¨¡å¼ - æœ€å¤§å¬å›"
            ),
            RetrievalStrategy(
                name="aggressive",
                vector_threshold=0.1,   # ğŸ”§ æ¿€è¿›æ¨¡å¼ä½é˜ˆå€¼ï¼Œå…œåº•ç­–ç•¥
                graph_threshold=0.05,   # ğŸ”§ æ¿€è¿›å›¾é˜ˆå€¼
                bm25_min_score=0.02,    # ğŸ”§ æ¿€è¿›BM25é˜ˆå€¼
                top_k=200,              # ğŸ”§ æœ€å¤šè¿”å›æ•°é‡
                description="æ¿€è¿›æ¨¡å¼ - å…œåº•ç­–ç•¥"
            )
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'strategy_usage': {s.name: 0 for s in self.strategies},
            'avg_results_per_query': 0.0
        }

    async def retrieve_with_fallback(self, query: str, **kwargs) -> Tuple[List[RetrievalResult], Dict[str, Any]]:
        """å¤šçº§å›é€€æ£€ç´¢"""
        self.stats['total_queries'] += 1
        
        try:
            # 1. æ™ºèƒ½æŸ¥è¯¢é¢„å¤„ç†
            processed_query = self.query_processor.process_query(query)
            logger.info(f"æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ: {processed_query.query_type}, ç½®ä¿¡åº¦: {processed_query.confidence}")
            
            # ğŸ”§ ä¿®å¤ï¼šç‰¹æ®ŠæŸ¥è¯¢ç±»å‹çš„ç›´æ¥å¤„ç†
            if processed_query.query_type == 'greeting':
                logger.info("æ£€æµ‹åˆ°é—®å€™è¯­ï¼Œè¿”å›å‹å¥½å›å¤")
                greeting_result = RetrievalResult(
                    content="ä½ å¥½ï¼æˆ‘æ˜¯AgenticX GraphRAGæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®æ‚¨æŸ¥è¯¢çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ",
                    score=1.0,
                    metadata={'type': 'greeting_response', 'search_source': 'system'}
                )
                return [greeting_result], {
                    'processed_query': processed_query,
                    'search_queries': [query],
                    'strategy_used': 'greeting_handler',
                    'total_results': 1,
                    'success': True
                }
            
            if processed_query.query_type == 'meaningless':
                logger.info("æ£€æµ‹åˆ°æ— æ„ä¹‰æŸ¥è¯¢ï¼Œè¿”å›æç¤º")
                help_result = RetrievalResult(
                    content="è¯·è¾“å…¥å…·ä½“çš„é—®é¢˜ï¼Œæˆ‘å¯ä»¥å¸®æ‚¨æŸ¥è¯¢ç›¸å…³ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼š\nâ€¢ æŸ¥è¯¢ç‰¹å®šæ¦‚å¿µæˆ–å®ä½“\nâ€¢ è¯¢é—®æŠ€æœ¯åŸç†\nâ€¢ äº†è§£äº§å“æœåŠ¡ç­‰",
                    score=1.0,
                    metadata={'type': 'help_response', 'search_source': 'system'}
                )
                return [help_result], {
                    'processed_query': processed_query,
                    'search_queries': [query],
                    'strategy_used': 'help_handler',
                    'total_results': 1,
                    'success': True
                }
            
            # 2. ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢
            search_queries = self.query_processor.generate_search_queries(processed_query)
            logger.info(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢: {search_queries}")
            
            # 3. æ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©èµ·å§‹ç­–ç•¥
            start_strategy_idx = self._select_start_strategy(processed_query)
            
            # 4. å¤šçº§å›é€€æ£€ç´¢
            all_results = []
            strategy_used = None
            
            for i in range(start_strategy_idx, len(self.strategies)):
                strategy = self.strategies[i]
                logger.info(f"å°è¯•ç­–ç•¥: {strategy.name} - {strategy.description}")
                
                # å¯¹æ¯ä¸ªæœç´¢æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢
                strategy_results = []
                for idx, search_query in enumerate(search_queries):
                    results = await self._execute_strategy(search_query, strategy)
                    logger.debug(f"æŸ¥è¯¢{idx+1}[{search_query[:30]}...] è¿”å› {len(results)} æ¡ç»“æœ")
                    strategy_results.extend(results)
                
                logger.info(f"ç­–ç•¥ {strategy.name} æ€»è®¡è·å¾— {len(strategy_results)} æ¡åŸå§‹ç»“æœ")
                
                # å»é‡å’Œæ’åº
                unique_results = self._deduplicate_results(strategy_results)
                logger.info(f"å»é‡åå‰©ä½™ {len(unique_results)} æ¡ç»“æœ")
                
                if unique_results:
                    all_results = unique_results
                    strategy_used = strategy
                    self.stats['strategy_usage'][strategy.name] += 1
                    logger.info(f"ç­–ç•¥ {strategy.name} æˆåŠŸï¼Œè¿”å› {len(unique_results)} æ¡ç»“æœ")
                    break
                else:
                    logger.info(f"ç­–ç•¥ {strategy.name} æ— ç»“æœï¼Œå°è¯•ä¸‹ä¸€çº§")
            
            # 5. å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œå°è¯•å®ä½“æœç´¢
            if not all_results:
                logger.warning("æ‰€æœ‰æ£€ç´¢ç­–ç•¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å®ä½“æœç´¢")
                entity_results = await self._direct_entity_search(processed_query)
                if entity_results:
                    all_results = entity_results
                    strategy_used = RetrievalStrategy("entity_search", 0, 0, 0, 10, "ç›´æ¥å®ä½“æœç´¢")
            
            # 6. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if all_results:
                self.stats['successful_queries'] += 1
                self.stats['avg_results_per_query'] = (
                    (self.stats['avg_results_per_query'] * (self.stats['total_queries'] - 1) + len(all_results)) 
                    / self.stats['total_queries']
                )
            
            # 7. ç”Ÿæˆæ£€ç´¢æŠ¥å‘Š
            retrieval_report = {
                'original_query': query,
                'processed_query': processed_query,
                'search_queries': search_queries,
                'strategy_used': strategy_used.name if strategy_used else 'none',
                'total_results': len(all_results),
                'success': len(all_results) > 0
            }
            
            return all_results, retrieval_report
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ£€ç´¢å¤±è´¥: {e}")
            return [], {'error': str(e), 'success': False}

    def _select_start_strategy(self, processed_query: ProcessedQuery) -> int:
        """æ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©èµ·å§‹ç­–ç•¥"""
        # ğŸ”§ ä¼˜åŒ–ï¼šå¯¹ç‰¹å®šæŸ¥è¯¢ç±»å‹ä½¿ç”¨æ›´å®½æ¾çš„ç­–ç•¥
        complex_query_types = ['specific_inquiry', 'commitment_inquiry', 'enumeration', 'classification', 'service_inquiry']
        
        # å¤æ‚æŸ¥è¯¢ç±»å‹ç›´æ¥ä»å®½æ¾æ¨¡å¼å¼€å§‹ï¼Œæé«˜å¬å›ç‡
        if processed_query.query_type in complex_query_types:
            return 2  # relaxed
        
        # é•¿æŸ¥è¯¢ï¼ˆé€šå¸¸åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼‰ä»å®½æ¾æ¨¡å¼å¼€å§‹
        if len(processed_query.original) > 20:
            return 2  # relaxed
        
        # åŒ…å«å¤šä¸ªå…³é”®è¯çš„æŸ¥è¯¢ä»æ ‡å‡†æ¨¡å¼å¼€å§‹
        if len(processed_query.keywords) >= 3:
            return 1  # standard
        
        # é«˜ç½®ä¿¡åº¦ä¸”æœ‰æ˜ç¡®å®ä½“çš„ç®€å•æŸ¥è¯¢ä»ä¸¥æ ¼æ¨¡å¼å¼€å§‹
        if processed_query.confidence > 0.8 and len(processed_query.entities) > 0 and len(processed_query.original) < 15:
            return 0  # strict
        
        # ä¸­ç­‰ç½®ä¿¡åº¦ä»æ ‡å‡†æ¨¡å¼å¼€å§‹
        elif processed_query.confidence > 0.6:
            return 1  # standard
        
        # å…¶ä»–æƒ…å†µä»å®½æ¾æ¨¡å¼å¼€å§‹ï¼Œç¡®ä¿æ›´å¥½çš„å¬å›ç‡
        else:
            return 2  # relaxed

    async def _execute_strategy(self, query: str, strategy: RetrievalStrategy) -> List[RetrievalResult]:
        """æ‰§è¡Œç‰¹å®šç­–ç•¥çš„æ£€ç´¢"""
        try:
            results = []
            
            # 1. æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25ï¼‰
            if hasattr(self.base_retriever, 'retrieve'):
                hybrid_results = await self.base_retriever.retrieve(
                    query, 
                    top_k=strategy.top_k,
                    min_score=strategy.vector_threshold
                )
                # ğŸ”§ ä¿®å¤ï¼šå¯¹å‘é‡æ£€ç´¢ç»“æœåº”ç”¨å‘é‡é˜ˆå€¼è¿‡æ»¤
                filtered_hybrid = [
                    r for r in hybrid_results 
                    if r.score >= strategy.vector_threshold
                ]
                # æ ‡è®°ç»“æœæ¥æº
                for result in filtered_hybrid:
                    if not hasattr(result, 'metadata') or result.metadata is None:
                        result.metadata = {}
                    result.metadata['source_type'] = 'vector'
                results.extend(filtered_hybrid)
            
            # 2. å›¾æ£€ç´¢
            if self.graph_retriever:
                try:
                    graph_results = await self.graph_retriever.retrieve(
                        query,
                        top_k=strategy.top_k // 2,
                        min_score=strategy.graph_threshold
                    )
                    # ğŸ”§ ä¿®å¤ï¼šå¯¹å›¾æ£€ç´¢ç»“æœåº”ç”¨å›¾é˜ˆå€¼è¿‡æ»¤
                    filtered_graph = [
                        r for r in graph_results 
                        if r.score >= strategy.graph_threshold
                    ]
                    # æ ‡è®°ç»“æœæ¥æº
                    for result in filtered_graph:
                        if not hasattr(result, 'metadata') or result.metadata is None:
                            result.metadata = {}
                        result.metadata['source_type'] = 'graph'
                    results.extend(filtered_graph)
                except Exception as e:
                    logger.warning(f"å›¾æ£€ç´¢å¤±è´¥: {e}")
            
            # 3. ğŸ”§ ä¿®å¤ï¼šæ ¹æ®ç»“æœæ¥æºåº”ç”¨ä¸åŒçš„æœ€ç»ˆè¿‡æ»¤æ ‡å‡†
            final_filtered = []
            for r in results:
                source_type = r.metadata.get('source_type', 'unknown') if hasattr(r, 'metadata') and r.metadata else 'unknown'
                
                if source_type == 'graph':
                    # å›¾æ£€ç´¢ç»“æœï¼šä½¿ç”¨å›¾é˜ˆå€¼
                    if r.score >= strategy.graph_threshold:
                        final_filtered.append(r)
                elif source_type == 'vector':
                    # å‘é‡æ£€ç´¢ç»“æœï¼šä½¿ç”¨å‘é‡é˜ˆå€¼
                    if r.score >= strategy.vector_threshold:
                        final_filtered.append(r)
                else:
                    # æœªçŸ¥æ¥æºï¼šä½¿ç”¨BM25é˜ˆå€¼ä½œä¸ºå…œåº•
                    if r.score >= strategy.bm25_min_score:
                        final_filtered.append(r)
            
            return final_filtered
            
        except Exception as e:
            logger.error(f"ç­–ç•¥æ‰§è¡Œå¤±è´¥: {e}")
            return []

    async def _direct_entity_search(self, processed_query: ProcessedQuery) -> List[RetrievalResult]:
        """ç›´æ¥å®ä½“æœç´¢ - ğŸš€ å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥"""
        try:
            if not self.storage_manager:
                return []
            
            from agenticx.storage import StorageType
            graph_storage = self.storage_manager.get_storage(StorageType.NEO4J)
            
            if not graph_storage:
                return []
            
            results = []
            
            # ğŸš€ ç­–ç•¥1: æœç´¢å®ä½“
            search_terms = processed_query.entities + processed_query.keywords + [processed_query.original]
            
            for term in search_terms:
                if len(term) < 1:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                    continue
                
                # ğŸš€ å¢å¼ºçš„æ¨¡ç³ŠåŒ¹é…å®ä½“æŸ¥è¯¢
                cypher_queries = [
                    # ç²¾ç¡®åŒ¹é…
                    """
                    MATCH (n:Entity) 
                    WHERE toLower(n.name) = toLower($term)
                    RETURN n.name as name, n.description as description, 
                           labels(n) as type, n.id as id, 1.0 as relevance
                    LIMIT 5
                    """,
                    # åŒ…å«åŒ¹é…
                    """
                    MATCH (n:Entity) 
                    WHERE toLower(n.name) CONTAINS toLower($term)
                       OR toLower($term) CONTAINS toLower(n.name)
                    RETURN n.name as name, n.description as description, 
                           labels(n) as type, n.id as id, 0.8 as relevance
                    LIMIT 10
                    """,
                    # æ­£åˆ™åŒ¹é…
                    """
                    MATCH (n:Entity) 
                    WHERE n.name =~ ('(?i).*' + $term + '.*')
                       OR n.description =~ ('(?i).*' + $term + '.*')
                    RETURN n.name as name, n.description as description, 
                           labels(n) as type, n.id as id, 0.6 as relevance
                    LIMIT 15
                    """,
                    # ğŸš€ æ–°å¢ï¼šæœç´¢æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬Chunkç­‰ï¼‰
                    """
                    MATCH (n) 
                    WHERE (n.name IS NOT NULL AND toLower(n.name) CONTAINS toLower($term))
                       OR (n.content IS NOT NULL AND toLower(n.content) CONTAINS toLower($term))
                       OR (n.description IS NOT NULL AND toLower(n.description) CONTAINS toLower($term))
                    RETURN n.name as name, 
                           COALESCE(n.description, n.content, '') as description, 
                           labels(n) as type, n.id as id, 0.4 as relevance
                    LIMIT 20
                    """
                ]
                
                for cypher_query in cypher_queries:
                    try:
                        entity_results = graph_storage.execute_query(
                            cypher_query, 
                            {"term": term}
                        )
                        
                        for result in entity_results:
                            if result['name']:  # ç¡®ä¿æœ‰åç§°
                                # è½¬æ¢ä¸ºRetrievalResultæ ¼å¼
                                content = f"å®ä½“: {result['name']}"
                                if result.get('description'):
                                    content += f"\næè¿°: {result['description'][:500]}..."  # é™åˆ¶æè¿°é•¿åº¦
                                
                                retrieval_result = RetrievalResult(
                                    content=content,
                                    score=result.get('relevance', 0.5),
                                    metadata={
                                        'type': 'entity',
                                        'entity_name': result['name'],
                                        'entity_type': result.get('type', []),
                                        'entity_id': result.get('id'),
                                        'search_source': 'direct_entity',
                                        'search_term': term
                                    },
                                    source='knowledge_graph'
                                )
                                results.append(retrieval_result)
                    
                    except Exception as e:
                        logger.warning(f"å®ä½“æŸ¥è¯¢å¤±è´¥: {e}")
                        continue
            
            # ğŸš€ ç­–ç•¥2: å¦‚æœå®ä½“æœç´¢æ— ç»“æœï¼Œå°è¯•å…¨æ–‡æœç´¢
            if not results:
                logger.info("å®ä½“æœç´¢æ— ç»“æœï¼Œå°è¯•å…¨æ–‡æœç´¢")
                results.extend(await self._full_text_search(processed_query, graph_storage))
            
            # ğŸš€ ç­–ç•¥3: å»é‡å’Œæ’åº
            unique_results = self._deduplicate_results(results)
            
            # ğŸš€ ç­–ç•¥4: å¦‚æœè¿˜æ˜¯æ— ç»“æœï¼Œè¿”å›ä¸€äº›é€šç”¨ä¿¡æ¯
            if not unique_results:
                logger.info("æ‰€æœ‰æœç´¢ç­–ç•¥æ— ç»“æœï¼Œè¿”å›ç³»ç»Ÿä¿¡æ¯")
                unique_results = await self._get_fallback_results(processed_query)
            
            return unique_results[:10]  # é™åˆ¶è¿”å›æ•°é‡
            
        except Exception as e:
            logger.error(f"ç›´æ¥å®ä½“æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _full_text_search(self, processed_query: ProcessedQuery, graph_storage) -> List[RetrievalResult]:
        """å…¨æ–‡æœç´¢"""
        results = []
        
        try:
            # æœç´¢æ‰€æœ‰åŒ…å«å…³é”®è¯çš„èŠ‚ç‚¹
            search_terms = [processed_query.original] + processed_query.keywords
            
            for term in search_terms:
                if len(term) < 2:
                    continue
                
                # å…¨æ–‡æœç´¢æŸ¥è¯¢
                cypher_query = """
                MATCH (n) 
                WHERE ANY(prop IN keys(n) WHERE toString(n[prop]) =~ ('(?i).*' + $term + '.*'))
                RETURN n, labels(n) as node_type, 
                       [prop IN keys(n) WHERE toString(n[prop]) =~ ('(?i).*' + $term + '.*')] as matched_props
                LIMIT 20
                """
                
                search_results = graph_storage.execute_query(cypher_query, {"term": term})
                
                for result in search_results:
                    node = result['n']
                    node_type = result['node_type']
                    matched_props = result['matched_props']
                    
                    # æ„å»ºå†…å®¹
                    content_parts = []
                    if hasattr(node, 'name') and node.name:
                        content_parts.append(f"åç§°: {node.name}")
                    if hasattr(node, 'content') and node.content:
                        content_parts.append(f"å†…å®¹: {node.content[:300]}...")
                    if hasattr(node, 'description') and node.description:
                        content_parts.append(f"æè¿°: {node.description[:200]}...")
                    
                    if content_parts:
                        retrieval_result = RetrievalResult(
                            content="\n".join(content_parts),
                            score=0.3,
                            metadata={
                                'type': 'full_text_match',
                                'node_type': node_type,
                                'matched_properties': matched_props,
                                'search_source': 'full_text',
                                'search_term': term
                            },
                            source='knowledge_graph'
                        )
                        results.append(retrieval_result)
        
        except Exception as e:
            logger.warning(f"å…¨æ–‡æœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def _get_fallback_results(self, processed_query: ProcessedQuery) -> List[RetrievalResult]:
        """è·å–å›é€€ç»“æœ"""
        fallback_results = []
        
        # æä¾›ä¸€äº›é€šç”¨çš„å¸®åŠ©ä¿¡æ¯
        fallback_content = f"""
å¾ˆæŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ "{processed_query.original}" ç›¸å…³çš„å…·ä½“ä¿¡æ¯ã€‚

ğŸ’¡ å»ºè®®æ‚¨å°è¯•ï¼š
1. ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯
2. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®
3. å°è¯•ç›¸å…³çš„åŒä¹‰è¯
4. ä½¿ç”¨æ›´ç®€å•çš„è¡¨è¾¾æ–¹å¼

ğŸ” ç³»ç»Ÿåˆ†æï¼š
- æŸ¥è¯¢ç±»å‹: {processed_query.query_type}
- è¯†åˆ«çš„å…³é”®è¯: {', '.join(processed_query.keywords) if processed_query.keywords else 'æ— '}
- è¯†åˆ«çš„å®ä½“: {', '.join(processed_query.entities) if processed_query.entities else 'æ— '}
- ç½®ä¿¡åº¦: {processed_query.confidence:.2f}
"""
        
        fallback_result = RetrievalResult(
            content=fallback_content.strip(),
            score=0.1,
            metadata={
                'type': 'fallback_help',
                'search_source': 'system_fallback',
                'original_query': processed_query.original
            },
            source='system'
        )
        
        fallback_results.append(fallback_result)
        
        return fallback_results

    def _deduplicate_results(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """å»é‡å’Œæ’åºç»“æœ - ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬ï¼šå‡å°‘è¿‡åº¦å»é‡"""
        if not results:
            return []
        
        logger.debug(f"å¼€å§‹å»é‡ï¼ŒåŸå§‹ç»“æœæ•°: {len(results)}")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šæ›´å®½æ¾çš„å»é‡ç­–ç•¥
        seen_chunk_ids = set()
        unique_results = []
        duplicate_count = 0
        
        for result in results:
            is_duplicate = False
            
            # ç­–ç•¥1: åŸºäºchunk_idå»é‡ï¼ˆæœ€ä¸¥æ ¼ï¼Œå®Œå…¨ç›¸åŒçš„å—ï¼‰
            if hasattr(result, 'chunk_id') and result.chunk_id:
                if result.chunk_id in seen_chunk_ids:
                    duplicate_count += 1
                    is_duplicate = True
                else:
                    seen_chunk_ids.add(result.chunk_id)
            
            # ç­–ç•¥2: åªå¯¹éå¸¸ç›¸ä¼¼çš„å†…å®¹è¿›è¡Œå»é‡ï¼ˆæé«˜é˜ˆå€¼åˆ°0.95ï¼‰
            if not is_duplicate:
                for existing in unique_results[-3:]:  # ğŸ”§ å‡å°‘æ£€æŸ¥èŒƒå›´ï¼Œåªæ£€æŸ¥æœ€è¿‘3ä¸ª
                    if self._is_content_similar(result.content, existing.content, threshold=0.95):
                        duplicate_count += 1
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                unique_results.append(result)
        
        logger.debug(f"å»é‡å®Œæˆ: ä¿ç•™ {len(unique_results)} æ¡ï¼Œå»é™¤ {duplicate_count} æ¡é‡å¤")
        
        # æŒ‰åˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x.score, reverse=True)
        
        return unique_results
    
    def _is_content_similar(self, content1: str, content2: str, threshold: float = 0.95) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå†…å®¹æ˜¯å¦ç›¸ä¼¼ - ğŸš€ ä¼˜åŒ–ï¼šæ›´ä¸¥æ ¼çš„ç›¸ä¼¼åº¦åˆ¤æ–­"""
        # ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„ç›¸ä¼¼åº¦è®¡ç®—
        
        # å¦‚æœå†…å®¹é•¿åº¦å·®å¼‚å¾ˆå¤§ï¼Œä¸å¤ªå¯èƒ½æ˜¯é‡å¤
        len1, len2 = len(content1), len(content2)
        if abs(len1 - len2) / max(len1, len2) > 0.3:  # é•¿åº¦å·®å¼‚è¶…è¿‡30%
            return False
        
        # è®¡ç®—è¯æ±‡é‡å åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return False
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        jaccard_similarity = intersection / union if union > 0 else 0
        
        # ğŸ”§ é¢å¤–æ£€æŸ¥ï¼šå¦‚æœå†…å®¹å¼€å¤´å’Œç»“å°¾éƒ½å¾ˆç›¸ä¼¼ï¼Œå¯èƒ½æ˜¯é‡å¤
        start_similar = content1[:100].lower() == content2[:100].lower()
        end_similar = content1[-100:].lower() == content2[-100:].lower()
        
        # åªæœ‰åœ¨Jaccardç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œæˆ–è€…å¼€å¤´ç»“å°¾éƒ½ç›¸åŒæ—¶æ‰è®¤ä¸ºæ˜¯é‡å¤
        return jaccard_similarity > threshold or (start_similar and end_similar and jaccard_similarity > 0.8)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (
            self.stats['successful_queries'] / self.stats['total_queries'] 
            if self.stats['total_queries'] > 0 else 0
        )
        
        return {
            'total_queries': self.stats['total_queries'],
            'successful_queries': self.stats['successful_queries'],
            'success_rate': success_rate,
            'avg_results_per_query': self.stats['avg_results_per_query'],
            'strategy_usage': self.stats['strategy_usage'],
            'most_used_strategy': max(
                self.stats['strategy_usage'].items(), 
                key=lambda x: x[1]
            )[0] if any(self.stats['strategy_usage'].values()) else 'none'
        }

    async def suggest_related_queries(self, query: str, max_suggestions: int = 5) -> List[str]:
        """åŸºäºæŸ¥è¯¢å¤±è´¥æƒ…å†µæä¾›ç›¸å…³æŸ¥è¯¢å»ºè®®"""
        try:
            processed_query = self.query_processor.process_query(query)
            suggestions = []
            
            # 1. åŸºäºå®ä½“çš„å»ºè®®
            for entity in processed_query.entities:
                suggestions.append(f"{entity}æ˜¯ä»€ä¹ˆ")
                suggestions.append(f"{entity}çš„ä½œç”¨")
                suggestions.append(f"{entity}çš„ç‰¹ç‚¹")
            
            # 2. åŸºäºå…³é”®è¯çš„å»ºè®®
            for keyword in processed_query.keywords:
                if len(keyword) > 2:
                    suggestions.append(f"{keyword}ç›¸å…³ä¿¡æ¯")
                    suggestions.append(f"å…³äº{keyword}")
            
            # 3. åŸºäºæŸ¥è¯¢ç±»å‹çš„å»ºè®®
            if processed_query.query_type == 'definition':
                suggestions.extend([
                    "ç›¸å…³æ¦‚å¿µä»‹ç»",
                    "åŸºæœ¬æ¦‚å¿µè¯´æ˜",
                    "æœ¯è¯­è§£é‡Š"
                ])
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_suggestions = []
            for suggestion in suggestions:
                if suggestion not in unique_suggestions and suggestion != query:
                    unique_suggestions.append(suggestion)
                if len(unique_suggestions) >= max_suggestions:
                    break
            
            return unique_suggestions
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŸ¥è¯¢å»ºè®®å¤±è´¥: {e}")
            return ["è¯·å°è¯•æ›´å…·ä½“çš„æŸ¥è¯¢", "ä½¿ç”¨å…³é”®è¯æœç´¢", "æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®"]


# ä½¿ç”¨ç¤ºä¾‹
async def test_enhanced_retriever():
    """æµ‹è¯•å¢å¼ºæ£€ç´¢å™¨"""
    # è¿™é‡Œéœ€è¦å®é™…çš„æ£€ç´¢å™¨å®ä¾‹
    # enhanced_retriever = EnhancedRetriever(base_retriever, graph_retriever, storage_manager)
    
    test_queries = [
        "é“å¡”æ˜¯å•¥",
        "ä¸­å›½é“å¡”å…¬å¸",
        "nihao",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯"
    ]
    
    print("å¢å¼ºæ£€ç´¢å™¨æµ‹è¯•")
    print("=" * 50)
    
    for query in test_queries:
        print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
        # results, report = await enhanced_retriever.retrieve_with_fallback(query)
        # print(f"ç»“æœæ•°é‡: {len(results)}")
        # print(f"ä½¿ç”¨ç­–ç•¥: {report.get('strategy_used', 'unknown')}")
        # print(f"æˆåŠŸ: {report.get('success', False)}")


if __name__ == "__main__":
    asyncio.run(test_enhanced_retriever())