import os
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'

#!/usr/bin/env python3
"""
AgenticX GraphRAG æ¼”ç¤ºç³»ç»Ÿä¸»ç¨‹åº

åŸºäº AgenticX æ ¸å¿ƒæ¨¡å—çš„å®Œæ•´ GraphRAG å®ç°ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£è§£æå’Œæ™ºèƒ½åˆ†å—
- å±‚çº§åŒ–çŸ¥è¯†å›¾è°±æ„å»º
- å¤šæ¨¡æ€å­˜å‚¨å’Œç´¢å¼•
- æ™ºèƒ½æ··åˆæ£€ç´¢
- äº¤äº’å¼é—®ç­”ç³»ç»Ÿ
"""
import absl.logging
absl.logging.set_verbosity('info')

import sys
import yaml
import asyncio
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
from loguru import logger

# è¿‡æ»¤è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=DeprecationWarning, module="importlib._bootstrap")
warnings.filterwarnings("ignore", message=".*datetime.datetime.utcnow.*")
warnings.filterwarnings("ignore", message=".*SwigPy.*")
warnings.filterwarnings("ignore", message=".*builtin type.*has no __module__ attribute.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="neo4j")
warnings.filterwarnings("ignore", message=".*session.*")
warnings.filterwarnings("ignore", message=".*connection.*")

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
# ç¡®ä¿ä»è„šæœ¬æ‰€åœ¨ç›®å½•åŠ è½½ .env æ–‡ä»¶
script_dir = Path(__file__).parent
env_path = script_dir / ".env"
load_dotenv(env_path)

# å¯¼å…¥ os æ¨¡å—ç”¨äºç¯å¢ƒå˜é‡
import os

# AgenticX æ ¸å¿ƒæ¨¡å—å¯¼å…¥
from agenticx.knowledge import (
    Knowledge,
    Document,
    DocumentProcessor,
    GraphRAGConstructor,
    KnowledgeGraphBuilder,
    SemanticChunker,
    AgenticChunker,
    get_chunker
)
from agenticx.knowledge.readers import (
    PDFReader,
    TextReader,
    JSONReader,
    CSVReader
)
from agenticx.embeddings import (
    EmbeddingRouter,
    OpenAIEmbeddingProvider,
    SiliconFlowEmbeddingProvider,
    BailianEmbeddingProvider
)
from agenticx.storage import (
    StorageManager,
    StorageConfig,
    StorageType,
    Neo4jStorage,
    ChromaStorage,
    RedisStorage,
    VectorRecord
)
from agenticx.retrieval import (
    HybridRetriever,
    GraphRetriever,
    VectorRetriever,
    BM25Retriever,
    AutoRetriever,
    Reranker,
    QueryAnalysisAgent,
    RetrievalAgent
)
from agenticx.llms import LlmFactory


class AgenticXGraphRAGDemo:
    """AgenticX GraphRAG æ¼”ç¤ºç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config_path: str = "configs.yml"):
        """åˆå§‹åŒ–æ¼”ç¤ºç³»ç»Ÿ"""
        self.config_path = config_path
        self.config = self._load_config()
        self.logger = self._setup_logging()
        
        # æ ¸å¿ƒç»„ä»¶
        self.llm_client = None
        self.embedding_router = None
        self.storage_manager = None
        self.knowledge_graph = None
        self.retriever = None
        
        # æ•°æ®è·¯å¾„
        self.data_dir = Path("./data")
        self.workspace_dir = Path(self.config['system']['workspace']['base_dir'])
        
        self.logger.info("AgenticX GraphRAG æ¼”ç¤ºç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path(self.config_path)
        if not config_file.exists():
            # å¦‚æœå½“å‰ç›®å½•æ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œå°è¯•ä»æ–‡æ¡£ç›®å½•åŠ è½½
            config_file = Path(".trae/documents/configs.yml")
            
        if not config_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path}")
            
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            
        # ç¯å¢ƒå˜é‡æ›¿æ¢
        self._replace_env_vars(config)
        
        return config
    
    def _replace_env_vars(self, obj: Any) -> None:
        """é€’å½’æ›¿æ¢é…ç½®ä¸­çš„ç¯å¢ƒå˜é‡"""
        if isinstance(obj, dict):
            for key, value in obj.items():
                if isinstance(value, str) and value.startswith('${') and value.endswith('}'):
                    env_var = value[2:-1]
                    obj[key] = os.getenv(env_var, value)
                else:
                    self._replace_env_vars(value)
        elif isinstance(obj, list):
            for item in obj:
                self._replace_env_vars(item)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # è¿‡æ»¤æ‰ç‰¹å®šçš„è­¦å‘Š
        warnings.filterwarnings("ignore", category=DeprecationWarning, module="importlib._bootstrap")
        warnings.filterwarnings("ignore", message=".*datetime.datetime.utcnow.*")
        warnings.filterwarnings("ignore", message=".*SwigPy.*")
        
        log_config = self.config.get('monitoring', {}).get('logging', {})
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path(self.config['system']['workspace']['logs_dir'])
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # ç§»é™¤é»˜è®¤çš„loguruå¤„ç†å™¨
        logger.remove()
        
        # é…ç½®æ§åˆ¶å°è¾“å‡º
        logger.add(
            sys.stdout,
            level=log_config.get('level', 'DEBUG'),  # ä¸´æ—¶å¯ç”¨DEBUGçº§åˆ«
            format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            colorize=True
        )
        
        # é…ç½®æ–‡ä»¶è¾“å‡º
        logger.add(
            log_dir / "agenticx_graphrag.log",
            level=log_config.get('level', 'INFO'),
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
            rotation="10 MB",
            retention="7 days",
            compression="zip"
        )
        
        return logger
    
    async def initialize_components(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒç»„ä»¶"""
        self.logger.info("æ­£åœ¨åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
        
        # 1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        await self._initialize_llm()
        
        # 2. åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        await self._initialize_embeddings()
        
        # 3. åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
        await self._initialize_storage()
        
        # 4. åˆå§‹åŒ–æ£€ç´¢å™¨
        await self._initialize_retriever()
        
        self.logger.info("æ‰€æœ‰æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    async def _initialize_llm(self) -> None:
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        from agenticx.knowledge.graphers.config import LLMConfig
        
        llm_config_dict = self.config['llm']
        provider = llm_config_dict.get('provider', 'litellm')
        
        # æä¾›å•†ç±»å‹æ˜ å°„
        provider_type_mapping = {
            'openai': 'litellm',
            'anthropic': 'litellm', 
            'litellm': 'litellm',
            'bailian': 'bailian',
            'kimi': 'kimi'
        }
        
        llm_type = provider_type_mapping.get(provider, 'litellm')
        
        # å°†å­—å…¸é…ç½®è½¬æ¢ä¸º LLMConfig å¯¹è±¡
        llm_config = LLMConfig(
            type=llm_type,
            model=llm_config_dict.get('model'),
            api_key=llm_config_dict.get('api_key'),
            base_url=llm_config_dict.get('base_url'),
            provider=provider,
            timeout=llm_config_dict.get('timeout'),
            max_retries=llm_config_dict.get('retry_attempts'),  # é…ç½®æ–‡ä»¶ä¸­æ˜¯retry_attempts
            temperature=llm_config_dict.get('temperature'),
            max_tokens=llm_config_dict.get('max_tokens')
        )
        
        self.llm_client = LlmFactory.create_llm(llm_config)
        self.logger.info(f"LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {llm_config.provider} - {llm_config.model}")
    
    async def _initialize_embeddings(self) -> None:
        """åˆå§‹åŒ–åµŒå…¥æœåŠ¡è·¯ç”±å™¨"""
        embed_config = self.config['embeddings']
        
        # åˆ›å»ºåµŒå…¥æä¾›å•†åˆ—è¡¨
        providers = []
        provider_names = []
        
        # æ ¹æ®é…ç½®çš„ä¼˜å…ˆçº§é¡ºåºæ·»åŠ æä¾›å•†
        router_config = embed_config.get('router', {})
        primary_provider = router_config.get('primary_provider', 'bailian')
        fallback_providers = router_config.get('fallback_providers', [])
        
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ·»åŠ æä¾›å•†
        provider_order = [primary_provider] + fallback_providers
        
        for provider_name in provider_order:
            if provider_name == 'bailian' and 'bailian' in embed_config:
                config = embed_config['bailian']
                provider = BailianEmbeddingProvider(
                    api_key=config['api_key'],
                    model=config.get('model', 'text-embedding-v4'),
                    api_url=config.get('base_url'),
                    dimensions=config.get('dimensions', 1536),
                    batch_size=config.get('batch_size', 10)  # ä¿®å¤ï¼šæ·»åŠ batch_sizeå‚æ•°
                )
                providers.append(provider)
                provider_names.append('bailian')
                
            elif provider_name == 'openai' and 'openai' in embed_config:
                config = embed_config['openai']
                provider = OpenAIEmbeddingProvider(
                    api_key=config['api_key'],
                    model=config.get('model', 'text-embedding-3-small'),
                    base_url=config.get('base_url'),
                    dimensions=config.get('dimensions', 1536)
                )
                providers.append(provider)
                provider_names.append('openai')
                
            elif provider_name == 'siliconflow' and 'siliconflow' in embed_config:
                config = embed_config['siliconflow']
                provider = SiliconFlowEmbeddingProvider(
                    api_key=config['api_key'],
                    model=config.get('model', 'BAAI/bge-large-zh-v1.5'),
                    dimensions=config.get('dimensions', 1024)
                )
                providers.append(provider)
                provider_names.append('siliconflow')
        
        # åˆ›å»ºè·¯ç”±å™¨ï¼ˆåªä¼ å…¥providersåˆ—è¡¨ï¼‰
        self.embedding_router = EmbeddingRouter(providers=providers)
        
        self.logger.info(f"åµŒå…¥æœåŠ¡è·¯ç”±å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒæä¾›å•†: {provider_names}")

    async def _initialize_storage(self) -> None:
        """åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨"""
        from agenticx.storage import StorageConfig, StorageType
        
        storage_config = self.config['storage']
        configs = []
        
        # åŠ¨æ€è·å–åµŒå…¥ç»´åº¦
        embedding_dim = self.embedding_router.get_embedding_dim()
        self.logger.info(f"åŠ¨æ€è·å–çš„åµŒå…¥ç»´åº¦: {embedding_dim}")

        # é…ç½®å‘é‡å­˜å‚¨
        vector_config = storage_config['vector']
        if vector_config['provider'] == 'milvus':
            try:
                milvus_config = vector_config.get('milvus', {})
                # æ„å»ºextra_paramsï¼Œè¿‡æ»¤æ‰Noneå€¼
                extra_params = {
                    'dimension': embedding_dim,
                    'collection_name': milvus_config.get('collection_name', 'agenticx_graphrag'),
                    'database': milvus_config.get('database', 'default'),
                    'recreate_if_exists': True  # ä¸´æ—¶é‡æ–°åˆ›å»ºä»¥åŒ¹é…æ–°çš„1024ç»´åº¦
                }
                
                # åªåœ¨usernameå’Œpasswordä¸ä¸ºNoneæ—¶æ‰æ·»åŠ 
                username = milvus_config.get('username')
                password = milvus_config.get('password')
                if username is not None:
                    extra_params['username'] = username
                if password is not None:
                    extra_params['password'] = password
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ‰€æœ‰å‚æ•°
                self.logger.info(f"ğŸ” Milvusé…ç½®å‚æ•°: host={milvus_config.get('host', 'localhost')}, port={milvus_config.get('port', 19530)}")
                self.logger.info(f"ğŸ” Milvus extra_params: {extra_params}")
                
                configs.append(StorageConfig(
                    storage_type=StorageType.MILVUS,
                    host=milvus_config.get('host', 'localhost'),
                    port=milvus_config.get('port', 19530),
                    extra_params=extra_params
                ))
            except Exception as e:
                self.logger.error(f"âŒ é…ç½®Milvuså­˜å‚¨å¤±è´¥: {e}")
                self.logger.warning(f"âš ï¸ Milvusé…ç½®å¤±è´¥: {e}ï¼Œå›é€€åˆ°Chroma")
                # å›é€€åˆ°Chroma
                chroma_config = vector_config.get('chroma', {})
                configs.append(StorageConfig(
                    storage_type=StorageType.CHROMA,
                    extra_params={
                        'dimension': embedding_dim,
                        'persist_directory': chroma_config.get('persist_directory', './storage/chroma'),
                        'collection_name': chroma_config.get('collection_name', 'agenticx_graphrag')
                    }
                ))
                self.logger.info("âœ… é…ç½®Chromaå‘é‡å­˜å‚¨ï¼ˆå›é€€ï¼‰")
        elif vector_config['provider'] == 'chroma':
            chroma_config = vector_config['chroma'].copy()
            configs.append(StorageConfig(
                storage_type=StorageType.CHROMA,
                extra_params={
                    'dimension': embedding_dim,
                    'persist_directory': chroma_config.get('persist_directory', './storage/chroma'),
                    'collection_name': chroma_config.get('collection_name', 'agenticx_graphrag')
                }
            ))
            self.logger.info("âœ… é…ç½®Chromaå‘é‡å­˜å‚¨")
        elif vector_config['provider'] == 'faiss':
            configs.append(StorageConfig(
                storage_type=StorageType.FAISS,
                extra_params={
                    'dimension': embedding_dim,  # ä½¿ç”¨åŠ¨æ€è·å–çš„ç»´åº¦
                    'index_path': './storage/faiss_index'
                }
            ))
            self.logger.info("âœ… é…ç½®FAISSå‘é‡å­˜å‚¨")
        
        # é…ç½®å›¾å­˜å‚¨
        graph_config = storage_config['graph']
        if graph_config['provider'] == 'neo4j':
            neo4j_config = graph_config['neo4j'].copy()
            
            # å¤„ç†URIæ ¼å¼çš„é…ç½®
            uri = neo4j_config.get('uri', 'bolt://localhost:7687')
            # å°†å®¹å™¨åneo4jæ›¿æ¢ä¸ºlocalhostï¼ˆç”¨äºå¤–éƒ¨è®¿é—®ï¼‰
            if 'neo4j:' in uri:
                uri = uri.replace('neo4j:', 'localhost:')
            
            configs.append(StorageConfig(
                storage_type=StorageType.NEO4J,
                connection_string=uri,
                username=neo4j_config.get('username', 'neo4j'),
                password=neo4j_config.get('password', None),
                database=neo4j_config.get('database', 'neo4j'),
                extra_params={
                    'max_connection_lifetime': neo4j_config.get('max_connection_lifetime', 3600),
                    'max_connection_pool_size': neo4j_config.get('max_connection_pool_size', 50),
                    'connection_acquisition_timeout': neo4j_config.get('connection_acquisition_timeout', 60)
                }
            ))
        
        # é…ç½®é”®å€¼å­˜å‚¨
        kv_config = storage_config['key_value']
        if kv_config['provider'] == 'redis':
            redis_config = kv_config['redis'].copy()
            configs.append(StorageConfig(
                storage_type=StorageType.REDIS,
                host=redis_config.pop('host', 'localhost'),
                port=redis_config.pop('port', 6379),
                password=redis_config.pop('password', None),
                database=redis_config.pop('database', 0),
                **redis_config
            ))
        elif kv_config['provider'] == 'sqlite':
            sqlite_config = kv_config.get('sqlite', {})
            configs.append(StorageConfig(
                storage_type=StorageType.SQLITE,
                connection_string=sqlite_config.get('database_path', './storage/cache.db')
            ))
            self.logger.info("âœ… é…ç½®SQLiteé”®å€¼å­˜å‚¨")
        
        # åˆ›å»ºå­˜å‚¨ç®¡ç†å™¨
        self.storage_manager = StorageManager(configs=configs)
        await self.storage_manager.initialize()
        
        # è°ƒè¯•ï¼šæ£€æŸ¥é”®å€¼å­˜å‚¨æ˜¯å¦å¯ç”¨
        kv_storage = await self.storage_manager.get_key_value_storage('default')
        if kv_storage:
            self.logger.info(f"âœ… é”®å€¼å­˜å‚¨å¯ç”¨: {type(kv_storage).__name__}")
        else:
            self.logger.warning("âš ï¸ é”®å€¼å­˜å‚¨ä¸å¯ç”¨")
    
    async def _initialize_retriever(self) -> None:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        from agenticx.storage import StorageType
        
        retrieval_config = self.config['retrieval']
        
        # åˆ›å»ºå‘é‡æ£€ç´¢å™¨
        vector_storage = self.storage_manager.get_storage(StorageType.MILVUS)
        vector_retriever = VectorRetriever(
            tenant_id="default",
            vector_storage=vector_storage,
            embedding_provider=self.embedding_router,
            **retrieval_config.get('vector', {})
        )
        
        # åˆ›å»ºBM25æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever(
            tenant_id="default",
            **retrieval_config.get('bm25', {})
        )
        
        # åˆ›å»ºå›¾æ£€ç´¢å™¨
        graph_storage = self.storage_manager.get_storage(StorageType.NEO4J)
        graph_retriever = GraphRetriever(
            tenant_id="default",
            graph_storage=graph_storage,
            **retrieval_config.get('graph', {})
        )
        
        # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
        from agenticx.retrieval.hybrid_retriever import HybridConfig
        hybrid_config = HybridConfig(**retrieval_config.get('hybrid', {}))
        self.retriever = HybridRetriever(
            vector_retriever=vector_retriever,
            bm25_retriever=bm25_retriever,
            config=hybrid_config
        )
        
        # å­˜å‚¨å›¾æ£€ç´¢å™¨ä»¥å¤‡åç”¨
        self.graph_retriever = graph_retriever
        
        # å¦‚æœé…ç½®äº†é‡æ’åºå™¨ï¼Œæ·»åŠ é‡æ’åºå™¨
        if 'reranker' in retrieval_config:
            reranker = Reranker(retrieval_config['reranker'])
            # æ³¨æ„ï¼šHybridRetriever å¯èƒ½æ²¡æœ‰ set_reranker æ–¹æ³•ï¼Œè¿™é‡Œå…ˆæ³¨é‡Šæ‰
            # self.retriever.set_reranker(reranker)
        
        self.logger.info("æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def validate_data_path(self) -> List[Path]:
        """éªŒè¯æ•°æ®è·¯å¾„å¹¶è¿”å›æ–‡ä»¶åˆ—è¡¨"""
        self.logger.info(f"éªŒè¯æ•°æ®è·¯å¾„: {self.data_dir}")
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.pdf', '.txt', '.json', '.csv', '.md'}
        
        # æ‰«ææ–‡ä»¶
        files = []
        for file_path in self.data_dir.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                files.append(file_path)
        
        if not files:
            raise ValueError(f"æ•°æ®ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {supported_extensions}")
        
        self.logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶: {[f.name for f in files]}")
        return files
    
    async def load_documents(self, file_paths: List[Path]) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        self.logger.info(f"å¼€å§‹åŠ è½½ {len(file_paths)} ä¸ªæ–‡æ¡£...")
        
        documents = []
        reader_config = self.config['knowledge']['readers']
        
        for file_path in file_paths:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–å™¨
                if file_path.suffix.lower() == '.pdf' and reader_config['pdf']['enabled']:
                    pdf_config = reader_config['pdf'].copy()
                    pdf_config.pop('enabled', None)  # ç§»é™¤enabledå­—æ®µ
                    reader = PDFReader(**pdf_config)
                elif file_path.suffix.lower() in ['.txt', '.md'] and reader_config['text']['enabled']:
                    text_config = reader_config['text'].copy()
                    text_config.pop('enabled', None)  # ç§»é™¤enabledå­—æ®µ
                    self.logger.debug(f"TextReaderé…ç½®: {text_config}")
                    reader = TextReader(**text_config)
                elif file_path.suffix.lower() == '.json' and reader_config['json']['enabled']:
                    json_config = reader_config['json'].copy()
                    json_config.pop('enabled', None)  # ç§»é™¤enabledå­—æ®µ
                    reader = JSONReader(**json_config)
                elif file_path.suffix.lower() == '.csv' and reader_config['csv']['enabled']:
                    csv_config = reader_config['csv'].copy()
                    csv_config.pop('enabled', None)  # ç§»é™¤enabledå­—æ®µ
                    reader = CSVReader(**csv_config)
                else:
                    self.logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹æˆ–å·²ç¦ç”¨: {file_path}")
                    continue
                
                # è¯»å–æ–‡æ¡£
                self.logger.debug(f"å¼€å§‹è¯»å–æ–‡æ¡£: {file_path}")
                result = await reader.read(str(file_path))
                self.logger.debug(f"è¯»å–ç»“æœç±»å‹: {type(result)}, å†…å®¹: {result if not isinstance(result, list) else f'åˆ—è¡¨é•¿åº¦: {len(result)}'}")
                
                # å¤„ç†è¿”å›ç»“æœï¼ˆå¯èƒ½æ˜¯å•ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£åˆ—è¡¨ï¼‰
                if isinstance(result, list):
                    # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œéå†æ¯ä¸ªæ–‡æ¡£
                    for doc in result:
                        documents.append(doc)
                        self.logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path.name} ({len(doc.content)} å­—ç¬¦)")
                else:
                    # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªæ–‡æ¡£
                    documents.append(result)
                    self.logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path.name} ({len(result.content)} å­—ç¬¦)")
                
            except Exception as e:
                self.logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
                continue
        
        self.logger.info(f"æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    async def build_knowledge_graph(self, documents: List[Document]) -> None:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        self.logger.info("å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # é…ç½® GraphRAG æ„é€ å™¨
        from agenticx.knowledge.graphers.config import LLMConfig, GraphRagConfig
        from agenticx.knowledge.base import ChunkingConfig
        
        graphrag_config_dict = self.config['knowledge']['graphrag']
        
        # è°ƒè¯•ï¼šæ‰“å°åŸå§‹é…ç½®
        self.logger.info(f"ğŸ” åŸå§‹GraphRAGé…ç½®å­—å…¸: {graphrag_config_dict}")
        self.logger.info(f"ğŸ” extraction_methodé…ç½®: {graphrag_config_dict.get('extraction_method', 'æœªè®¾ç½®')}")
        
        # å°†å­—å…¸è½¬æ¢ä¸º GraphRagConfig å¯¹è±¡
        graphrag_config = GraphRagConfig.from_dict(graphrag_config_dict)
        self.logger.info(f"ğŸ” è½¬æ¢åçš„GraphRAGé…ç½®: extraction_method={getattr(graphrag_config, 'extraction_method', 'æœªè®¾ç½®')}")
        self.logger.debug(f"GraphRAGé…ç½®: {graphrag_config}")
        llm_config_dict = self.config['llm']
        
        # è½¬æ¢ä¸º LLMConfig å¯¹è±¡
        provider = llm_config_dict.get('provider', 'litellm')
        
        # æä¾›å•†ç±»å‹æ˜ å°„
        provider_type_mapping = {
            'openai': 'litellm',
            'anthropic': 'litellm', 
            'litellm': 'litellm',
            'bailian': 'bailian',
            'kimi': 'kimi'
        }
        
        llm_type = provider_type_mapping.get(provider, 'litellm')
        
        # æ‰‹åŠ¨åˆ›å»º LLMConfigï¼Œç¡®ä¿ type å­—æ®µæ­£ç¡®
        llm_config = LLMConfig(
            type=llm_type,
            model=llm_config_dict.get('model'),
            api_key=llm_config_dict.get('api_key'),
            base_url=llm_config_dict.get('base_url'),
            provider=provider,
            timeout=llm_config_dict.get('timeout'),
            max_retries=llm_config_dict.get('retry_attempts'),  # é…ç½®æ–‡ä»¶ä¸­æ˜¯retry_attempts
            temperature=llm_config_dict.get('temperature'),
            max_tokens=llm_config_dict.get('max_tokens')
        )
        
        # åˆ›å»ºå¼ºæ¨¡å‹é…ç½®ï¼ˆç”¨äºæ–‡æ¡£åˆ†æå’ŒSchemaç”Ÿæˆï¼‰
        strong_model_dict = llm_config_dict.get('strong_model', llm_config_dict)
        strong_provider = strong_model_dict.get('provider', provider)
        strong_llm_type = provider_type_mapping.get(strong_provider, 'litellm')
        
        strong_llm_config = LLMConfig(
            type=strong_llm_type,
            model=strong_model_dict.get('model'),
            api_key=strong_model_dict.get('api_key'),
            base_url=strong_model_dict.get('base_url'),
            provider=strong_provider,
            timeout=strong_model_dict.get('timeout'),
            max_retries=strong_model_dict.get('retry_attempts', 3),
            temperature=strong_model_dict.get('temperature'),
            max_tokens=strong_model_dict.get('max_tokens')
        )
        
        # å°†å¼ºæ¨¡å‹é…ç½®æ·»åŠ åˆ°graphrag_configä¸­
        graphrag_config.strong_model_config = strong_llm_config
        
        self.logger.info(f"ğŸ¤– é»˜è®¤æ¨¡å‹: {llm_config.model}")
        self.logger.info(f"ğŸš€ å¼ºæ¨¡å‹: {strong_llm_config.model}")
        
        self.logger.debug(f"GraphRAG LLMConfig: type={llm_config.type}, provider={llm_config.provider}, model={llm_config.model}")
        
        # ä½¿ç”¨åˆ†å—å™¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—
        chunking_config_dict = self.config['knowledge']['chunking']
        strategy = chunking_config_dict.get('strategy', 'semantic')
        chunk_size = chunking_config_dict.get('chunk_size', 800)
        chunk_overlap = chunking_config_dict.get('chunk_overlap', 150)
        
        chunking_config = ChunkingConfig(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.logger.info(f"ä½¿ç”¨åˆ†å—ç­–ç•¥: {strategy}, åˆ†å—å¤§å°: {chunk_size}")
        
        # å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†å—
        all_chunks = []
        for i, document in enumerate(documents):
            self.logger.debug(f"æ­£åœ¨åˆ†å—æ–‡æ¡£ {i+1}/{len(documents)}: {document.metadata.name}")
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦éœ€è¦åˆ†å—
            if len(document.content) > chunk_size:
                # ä½¿ç”¨åˆ†å—å™¨ï¼Œç¡®ä¿æ‰€æœ‰ç­–ç•¥éƒ½èƒ½è·å¾—åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºåç»­å‘é‡åŒ–ï¼‰
                chunker_kwargs = {
                    'embedding_model': self.embedding_router  # æ‰€æœ‰åˆ†å—å™¨éƒ½éœ€è¦åµŒå…¥æ¨¡å‹
                }
                
                if strategy == "semantic":
                    chunker_kwargs['similarity_threshold'] = chunking_config_dict.get('semantic', {}).get('similarity_threshold', 0.8)
                    chunker_kwargs['min_chunk_size'] = chunking_config_dict.get('min_chunk_size', 100)
                    chunker_kwargs['max_chunk_size'] = chunking_config_dict.get('max_chunk_size', 1200)
                    self.logger.info(f"ğŸ” å¯ç”¨è¯­ä¹‰åˆ†å—ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {chunker_kwargs['similarity_threshold']}")
                elif strategy == "agentic":
                    chunker_kwargs['llm_client'] = self.llm_client
                    agentic_config = chunking_config_dict.get('agentic', {})
                    chunker_kwargs.update(agentic_config)
                    self.logger.info("ğŸ¤– å¯ç”¨Agenticæ™ºèƒ½åˆ†å—")
                elif strategy == "fixed_size":
                    self.logger.info("ğŸ“ ä½¿ç”¨å›ºå®šå¤§å°åˆ†å—ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰")
                
                chunker = get_chunker(strategy, **chunker_kwargs)
                
                # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºåˆ†å—å™¨ç±»å‹å’Œé…ç½®
                self.logger.info(f"ğŸ”§ åˆ†å—å™¨ç±»å‹: {type(chunker).__name__}")
                if hasattr(chunker, 'embedding_model') and chunker.embedding_model:
                    self.logger.info(f"ğŸ¤– åµŒå…¥æ¨¡å‹: {type(chunker.embedding_model).__name__}")
                    self.logger.info(f"ğŸ“Š ç›¸ä¼¼åº¦é˜ˆå€¼: {getattr(chunker, 'similarity_threshold', 'N/A')}")
                else:
                    self.logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°åµŒå…¥æ¨¡å‹ï¼Œå¯èƒ½ä½¿ç”¨å›é€€ç­–ç•¥")
                
                self.logger.info(f"ğŸ“„ å¼€å§‹åˆ†å—æ–‡æ¡£: {document.metadata.name} (é•¿åº¦: {len(document.content)} å­—ç¬¦)")
                chunks = await chunker.chunk_document_async(document)
                
                # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºåˆ†å—ç»“æœ
                if hasattr(chunks, 'strategy_used'):
                    self.logger.info(f"ğŸ¯ å®é™…ä½¿ç”¨çš„åˆ†å—ç­–ç•¥: {chunks.strategy_used}")
                if hasattr(chunks, 'metadata') and chunks.metadata:
                    metadata = chunks.metadata
                    if 'original_sentences' in metadata:
                        self.logger.info(f"ğŸ“ åŸå§‹å¥å­æ•°: {metadata['original_sentences']}")
                    if 'similarity_threshold' in metadata:
                        self.logger.info(f"ğŸšï¸ ç›¸ä¼¼åº¦é˜ˆå€¼: {metadata['similarity_threshold']}")
                
                self.logger.info(f"âœ… åˆ†å—å®Œæˆ: {len(chunks.chunks)} ä¸ªå—")
                
                if hasattr(chunks, 'chunks'):
                    # å¦‚æœè¿”å›çš„æ˜¯ChunkingResultå¯¹è±¡
                    chunk_docs = chunks.chunks
                else:
                    # å¦‚æœç›´æ¥è¿”å›æ–‡æ¡£åˆ—è¡¨
                    chunk_docs = chunks
                
                all_chunks.extend(chunk_docs)
                self.logger.info(f"æ–‡æ¡£ {document.metadata.name} è¢«åˆ†æˆ {len(chunk_docs)} ä¸ªå—")
            else:
                # æ–‡æ¡£è¶³å¤Ÿå°ï¼Œä¸éœ€è¦åˆ†å—
                all_chunks.append(document)
                self.logger.debug(f"æ–‡æ¡£ {document.metadata.name} æ— éœ€åˆ†å—")
        
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œå…± {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ä½¿ç”¨æ–°çš„KnowledgeGraphBuilderè¿›è¡Œä¸¤é˜¶æ®µSPOæŠ½å–
        builder = KnowledgeGraphBuilder(
            config=graphrag_config,
            llm_config=llm_config
        )
        
        # æå–åˆ†å—åçš„æ–‡æ¡£æ–‡æœ¬
        texts = [chunk.content for chunk in all_chunks]
        metadata = [chunk.metadata.__dict__ for chunk in all_chunks]
        
        # æ„å»ºå›¾è°±ï¼ˆä½¿ç”¨æ‰¹å¤„ç†SPOæŠ½å–ï¼‰
        self.knowledge_graph = await builder.build_from_texts(
            texts=texts,
            metadata=metadata
        )
        
        self.logger.info(
            f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: "
            f"{len(self.knowledge_graph.entities)} ä¸ªå®ä½“, "
            f"{len(self.knowledge_graph.relationships)} ä¸ªå…³ç³»"
        )
    
    async def store_and_index(self) -> None:
        """å­˜å‚¨å’Œç´¢å¼•çŸ¥è¯†å›¾è°±"""
        self.logger.info("å¼€å§‹å­˜å‚¨å’Œç´¢å¼•çŸ¥è¯†å›¾è°±...")
        
        # 1. å­˜å‚¨åˆ°å›¾æ•°æ®åº“
        try:
            self.logger.debug("ğŸ” å¼€å§‹æŸ¥æ‰¾å›¾æ•°æ®åº“å­˜å‚¨...")
            graph_storage = await self.storage_manager.get_graph_storage('default')
            
            if graph_storage:
                self.logger.info(f"âœ… æ‰¾åˆ°å›¾å­˜å‚¨: {type(graph_storage).__name__}")
                graph_storage.store_graph(self.knowledge_graph, clear_existing=True) # ç¡®ä¿æ¸…ç©º
                self.logger.info("çŸ¥è¯†å›¾è°±å·²å­˜å‚¨åˆ°å›¾æ•°æ®åº“")
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å›¾æ•°æ®åº“å­˜å‚¨ï¼Œè·³è¿‡å›¾è°±å­˜å‚¨")
                # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ‰€æœ‰å­˜å‚¨
                self.logger.debug(f"ğŸ“Š å½“å‰å­˜å‚¨å®ä¾‹æ•°é‡: {len(self.storage_manager.storages)}")
                for i, storage in enumerate(self.storage_manager.storages):
                    storage_type = type(storage).__name__
                    has_store_graph = hasattr(storage, 'store_graph')
                    has_add_triplet = hasattr(storage, 'add_triplet')
                    has_add_node = hasattr(storage, 'add_node')
                    self.logger.debug(f"  å­˜å‚¨ {i+1}: {storage_type} - store_graph:{has_store_graph}, add_triplet:{has_add_triplet}, add_node:{has_add_node}")
        except Exception as e:
            self.logger.error(f"âŒ å›¾æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}")
            import traceback
            self.logger.debug(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            self.logger.warning("ğŸ’¡ ç»§ç»­æ‰§è¡Œå…¶ä»–ç´¢å¼•æ­¥éª¤...")
        
        # 2. æ„å»ºå‘é‡ç´¢å¼•
        try:
            await self._build_vector_index()
        except Exception as e:
            self.logger.error(f"âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        
        # 3. æ„å»º SPO ç´¢å¼•
        try:
            await self._build_spo_index()
        except Exception as e:
            self.logger.error(f"âŒ SPOç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        
        # 4. ç¼“å­˜å…³é”®æ•°æ®
        try:
            await self._cache_key_data()
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®ç¼“å­˜å¤±è´¥: {e}")
        
        self.logger.info("å­˜å‚¨å’Œç´¢å¼•å®Œæˆ")
    
    async def _build_vector_index(self) -> None:
        """æ„å»ºå‘é‡ç´¢å¼•"""
        from agenticx.storage import StorageType
        
        self.logger.info("æ„å»ºå‘é‡ç´¢å¼•...")
        
        # å°è¯•è·å–å‘é‡å­˜å‚¨
        vector_storage = await self.storage_manager.get_vector_storage('default')
        if not vector_storage:
            # å›é€€åˆ°é€šè¿‡ç±»å‹è·å–
            vector_storage = self.storage_manager.get_storage(StorageType.CHROMA)
            if not vector_storage:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å‘é‡å­˜å‚¨ï¼Œè·³è¿‡å‘é‡ç´¢å¼•æ„å»º")
                return
        
        # ä¸ºå®ä½“æ„å»ºå‘é‡ç´¢å¼•
        entity_records = []
        for entity in self.knowledge_graph.entities.values():
            # ç”Ÿæˆå®ä½“æè¿°æ–‡æœ¬
            entity_text = f"{entity.name}: {entity.description or ''}"
            
            # ç”ŸæˆåµŒå…¥
            embedding = await self.embedding_router.aembed_text(entity_text)
            
            # åˆ›å»ºå‘é‡è®°å½•
            record = VectorRecord(
                id=entity.id,
                vector=embedding,
                metadata={
                    'type': 'entity',
                    'entity_type': entity.entity_type.value,
                    'name': entity.name,
                    'confidence': entity.confidence
                },
                content=entity_text
            )
            entity_records.append(record)
        
        # æ‰¹é‡å­˜å‚¨
        vector_storage.add(entity_records)
        self.logger.info(f"å®ä½“å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ: {len(entity_records)} æ¡è®°å½•")
        
        # ä¸ºå…³ç³»æ„å»ºå‘é‡ç´¢å¼•
        relationship_records = []
        for relationship in self.knowledge_graph.relationships.values():
            # ç”Ÿæˆå…³ç³»æè¿°æ–‡æœ¬
            source_entity = self.knowledge_graph.get_entity(relationship.source_entity_id)
            target_entity = self.knowledge_graph.get_entity(relationship.target_entity_id)
            
            if source_entity and target_entity:
                rel_text = f"{source_entity.name} {relationship.relation_type.value} {target_entity.name}"
                
                # ç”ŸæˆåµŒå…¥
                embedding = await self.embedding_router.aembed_text(rel_text)
                
                # åˆ›å»ºå‘é‡è®°å½•
                record = VectorRecord(
                    id=relationship.id,
                    vector=embedding,
                    metadata={
                        'type': 'relationship',
                        'relation_type': relationship.relation_type.value,
                        'source_entity': source_entity.name,
                        'target_entity': target_entity.name,
                        'confidence': relationship.confidence
                    },
                    content=rel_text
                )
                relationship_records.append(record)
        
        vector_storage.add(relationship_records)
        self.logger.info(f"å…³ç³»å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ: {len(relationship_records)} æ¡è®°å½•")
    
    async def _build_spo_index(self) -> None:
        """æ„å»º SPO ä¸‰å…ƒç»„ç´¢å¼•"""
        import json
        self.logger.info("æ„å»º SPO ä¸‰å…ƒç»„ç´¢å¼•...")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥å­˜å‚¨ç®¡ç†å™¨çŠ¶æ€
        self.logger.info(f"å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–çŠ¶æ€: {self.storage_manager.initialized}")
        self.logger.info(f"å­˜å‚¨å®ä¾‹æ•°é‡: {len(self.storage_manager.storages)}")
        
        kv_storage = await self.storage_manager.get_key_value_storage('default')
        self.logger.info(f"è·å–åˆ°çš„é”®å€¼å­˜å‚¨: {type(kv_storage) if kv_storage else 'None'}")
        
        if not kv_storage:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°é”®å€¼å­˜å‚¨ï¼Œè·³è¿‡SPOç´¢å¼•æ„å»º")
            return
        
        # æ„å»º SPO ç´¢å¼•
        spo_index = {
            'subject_index': {},  # ä¸»è¯­ç´¢å¼•
            'predicate_index': {},  # è°“è¯­ç´¢å¼•
            'object_index': {}  # å®¾è¯­ç´¢å¼•
        }
        
        for relationship in self.knowledge_graph.relationships.values():
            source_entity = self.knowledge_graph.get_entity(relationship.source_entity_id)
            target_entity = self.knowledge_graph.get_entity(relationship.target_entity_id)
            
            if source_entity and target_entity:
                subject = source_entity.name
                predicate = relationship.relation_type.value
                object_name = target_entity.name
                
                # ä¸»è¯­ç´¢å¼•
                if subject not in spo_index['subject_index']:
                    spo_index['subject_index'][subject] = []
                spo_index['subject_index'][subject].append({
                    'predicate': predicate,
                    'object': object_name,
                    'relationship_id': relationship.id
                })
                
                # è°“è¯­ç´¢å¼•
                if predicate not in spo_index['predicate_index']:
                    spo_index['predicate_index'][predicate] = []
                spo_index['predicate_index'][predicate].append({
                    'subject': subject,
                    'object': object_name,
                    'relationship_id': relationship.id
                })
                
                # å®¾è¯­ç´¢å¼•
                if object_name not in spo_index['object_index']:
                    spo_index['object_index'][object_name] = []
                spo_index['object_index'][object_name].append({
                    'subject': subject,
                    'predicate': predicate,
                    'relationship_id': relationship.id
                })
        
        # å­˜å‚¨ç´¢å¼•ï¼ˆåºåˆ—åŒ–ä¸ºJSONï¼‰
        kv_storage.set('spo_index', json.dumps(spo_index, ensure_ascii=False))
        
        self.logger.info(
            f"SPO ç´¢å¼•æ„å»ºå®Œæˆ: "
            f"{len(spo_index['subject_index'])} ä¸ªä¸»è¯­, "
            f"{len(spo_index['predicate_index'])} ä¸ªè°“è¯­, "
            f"{len(spo_index['object_index'])} ä¸ªå®¾è¯­"
        )
    
    async def _cache_key_data(self) -> None:
        """ç¼“å­˜å…³é”®æ•°æ®"""
        import json
        from datetime import datetime, timezone
        self.logger.info("ç¼“å­˜å…³é”®æ•°æ®...")
        
        kv_storage = await self.storage_manager.get_key_value_storage('default')
        if not kv_storage:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°é”®å€¼å­˜å‚¨ï¼Œè·³è¿‡æ•°æ®ç¼“å­˜")
            return
        
        # ç¼“å­˜å›¾è°±ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'entity_count': len(self.knowledge_graph.entities),
            'relationship_count': len(self.knowledge_graph.relationships),
            'entity_types': {},
            'relationship_types': {},
            'build_time': datetime.now(timezone.utc).isoformat()
        }
        
        # ç»Ÿè®¡å®ä½“ç±»å‹
        for entity in self.knowledge_graph.entities.values():
            entity_type = entity.entity_type.value
            stats['entity_types'][entity_type] = stats['entity_types'].get(entity_type, 0) + 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹
        for relationship in self.knowledge_graph.relationships.values():
            rel_type = relationship.relation_type.value
            stats['relationship_types'][rel_type] = stats['relationship_types'].get(rel_type, 0) + 1
        
        kv_storage.set('graph_stats', json.dumps(stats, ensure_ascii=False))
        self.logger.info("å…³é”®æ•°æ®ç¼“å­˜å®Œæˆ")
    
    async def interactive_qa(self) -> None:
        """äº¤äº’å¼é—®ç­”ç³»ç»Ÿ"""
        self.logger.info("å¯åŠ¨äº¤äº’å¼é—®ç­”ç³»ç»Ÿ...")
        
        print("\n" + "="*60)
        print("ğŸ¤– AgenticX GraphRAG é—®ç­”ç³»ç»Ÿ")
        print("="*60)
        print("æ”¯æŒçš„æŸ¥è¯¢ç±»å‹:")
        print("  1. å®ä½“æŸ¥è¯¢: æŸ¥æ‰¾ç‰¹å®šå®ä½“çš„ä¿¡æ¯")
        print("  2. å…³ç³»æŸ¥è¯¢: æŸ¥æ‰¾å®ä½“é—´çš„å…³ç³»")
        print("  3. è·¯å¾„æŸ¥è¯¢: æŸ¥æ‰¾å®ä½“é—´çš„è¿æ¥è·¯å¾„")
        print("  4. ç¤¾åŒºæŸ¥è¯¢: æŸ¥æ‰¾ç›¸å…³å®ä½“ç¾¤ç»„")
        print("  5. è‡ªç”±é—®ç­”: åŸºäºçŸ¥è¯†å›¾è°±çš„å¼€æ”¾å¼é—®ç­”")
        print("\nè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
        print("="*60 + "\n")
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                query = input("ğŸ” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ AgenticX GraphRAG ç³»ç»Ÿï¼")
                    break
                
                if not query:
                    continue
                
                # å¤„ç†æŸ¥è¯¢
                await self._process_query(query)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ AgenticX GraphRAG ç³»ç»Ÿï¼")
                break
            except Exception as e:
                self.logger.error(f"æŸ¥è¯¢å¤„ç†é”™è¯¯: {e}")
                print(f"âŒ æŸ¥è¯¢å¤„ç†å‡ºé”™: {e}")
    
    async def _process_query(self, query: str) -> None:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç†æŸ¥è¯¢: {query}")
        
        try:
            # è·å–æ£€ç´¢é…ç½®
            retrieval_config = self.config.get('retrieval', {})
            graph_config = retrieval_config.get('graph', {})
            vector_config = retrieval_config.get('vector', {})
            
            # å…ˆå°è¯•å›¾æ£€ç´¢ï¼ˆé€‚åˆå®ä½“æŸ¥è¯¢ï¼‰
            graph_results = []
            if hasattr(self, 'graph_retriever') and self.graph_retriever:
                try:
                    graph_top_k = min(graph_config.get('max_nodes', 50), 10)  # é™åˆ¶åœ¨10ä»¥å†…
                    graph_results = await self.graph_retriever.retrieve(query, top_k=graph_top_k)
                    if graph_results:
                        print(f"ğŸ” å›¾æ£€ç´¢æ‰¾åˆ° {len(graph_results)} æ¡ç»“æœ (top_k={graph_top_k})")
                except Exception as e:
                    self.logger.warning(f"å›¾æ£€ç´¢å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ··åˆæ£€ç´¢å™¨è¿›è¡ŒæŸ¥è¯¢
            hybrid_top_k = vector_config.get('top_k', 20)
            hybrid_results = await self.retriever.retrieve(query, top_k=hybrid_top_k)
            print(f"ğŸ” æ··åˆæ£€ç´¢æ‰¾åˆ° {len(hybrid_results)} æ¡ç»“æœ (top_k={hybrid_top_k})")
            
            # åˆå¹¶ç»“æœ
            all_results = graph_results + hybrid_results
            
            # å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
            seen_ids = set()
            unique_results = []
            for result in all_results:
                result_id = getattr(result, 'id', str(hash(result.content)))
                if result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            unique_results.sort(key=lambda x: getattr(x, 'score', 0), reverse=True)
            results = unique_results[:5]  # å–å‰5ä¸ª
            
            if not results:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                # å°è¯•ç›´æ¥åœ¨Neo4jä¸­æœç´¢å®ä½“
                await self._search_entity_directly(query)
                return
            
            # æ˜¾ç¤ºæ£€ç´¢é…ç½®ä¿¡æ¯
            similarity_threshold = vector_config.get('similarity_threshold', 0.7)
            print(f"\nâœ… æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ä¿¡æ¯ (ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}):\n")
            
            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
            for i, result in enumerate(results, 1):
                score = getattr(result, 'score', 0)
                score_status = "âœ…" if score >= similarity_threshold else "âš ï¸"
                print(f"ğŸ“„ ç»“æœ {i} {score_status} (ç›¸ä¼¼åº¦: {score:.3f})")
                print(f"   å†…å®¹: {result.content[:200]}...")
                if result.metadata:
                    print(f"   å…ƒæ•°æ®: {result.metadata}")
                print()
            
            # ç”Ÿæˆç­”æ¡ˆ
            await self._generate_answer(query, results)
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¤„ç†é”™è¯¯: {e}")
            print(f"âŒ æŸ¥è¯¢å¤„ç†å‡ºé”™: {e}")
    
    async def _search_entity_directly(self, query: str) -> None:
        """ç›´æ¥åœ¨Neo4jä¸­æœç´¢å®ä½“"""
        try:
            from agenticx.storage import StorageType
            graph_storage = self.storage_manager.get_storage(StorageType.NEO4J)
            
            if not graph_storage:
                print("âš ï¸ å›¾å­˜å‚¨ä¸å¯ç”¨")
                return
            
            # æå–å¯èƒ½çš„å®ä½“åç§°
            entity_name = query.replace("æ˜¯å•¥", "").replace("æ˜¯ä»€ä¹ˆ", "").replace("?", "").replace("ï¼Ÿ", "").strip()
            
            # åœ¨Neo4jä¸­æœç´¢å®ä½“
            cypher_query = """
            MATCH (n:Entity) 
            WHERE n.name CONTAINS $entity_name OR n.name =~ ('(?i).*' + $entity_name + '.*')
            RETURN n.name as name, n.description as description, labels(n) as type
            LIMIT 5
            """
            
            results = graph_storage.execute_query(cypher_query, {"entity_name": entity_name})
            
            if results:
                print(f"\nğŸ” åœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°ç›¸å…³å®ä½“:")
                for result in results:
                    print(f"ğŸ“ å®ä½“: {result['name']}")
                    print(f"   ç±»å‹: {result['type']}")
                    if result['description']:
                        print(f"   æè¿°: {result['description']}")
                    print()
            else:
                print(f"âŒ åœ¨çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ° '{entity_name}' ç›¸å…³çš„å®ä½“")
                
        except Exception as e:
            self.logger.error(f"ç›´æ¥å®ä½“æœç´¢å¤±è´¥: {e}")
            print(f"âŒ å®ä½“æœç´¢å‡ºé”™: {e}")
    
    async def _generate_answer(self, query: str, results: List[Any]) -> None:
        """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n".join([result.content for result in results[:3]])
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„ç­”æ¡ˆã€‚

çŸ¥è¯†å›¾è°±ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {query}

è¯·å›ç­”:
"""
            
            # è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
            response = await self.llm_client.ainvoke(prompt)
            
            print("ğŸ¤– AI å›ç­”:")
            print("-" * 40)
            print(response.content)
            print("-" * 40)
            
        except Exception as e:
            self.logger.error(f"ç­”æ¡ˆç”Ÿæˆé”™è¯¯: {e}")
            print(f"âŒ ç­”æ¡ˆç”Ÿæˆå‡ºé”™: {e}")
    
    async def run_demo(self) -> None:
        """è¿è¡Œå®Œæ•´æ¼”ç¤ºæµç¨‹"""
        try:
            print("ğŸš€ å¯åŠ¨ AgenticX GraphRAG æ¼”ç¤ºç³»ç»Ÿ...")
            
            # 1. åˆå§‹åŒ–ç»„ä»¶
            await self.initialize_components()
            
            # 2. éªŒè¯æ•°æ®è·¯å¾„
            file_paths = self.validate_data_path()
            
            # 3. åŠ è½½æ–‡æ¡£
            documents = await self.load_documents(file_paths)
            
            # 4. æ„å»ºçŸ¥è¯†å›¾è°±
            await self.build_knowledge_graph(documents)
            
            # 5. å­˜å‚¨å’Œç´¢å¼•
            await self.store_and_index()
            
            # 6. å¯åŠ¨äº¤äº’å¼é—®ç­”
            await self.interactive_qa()
            
        except Exception as e:
            self.logger.error(f"æ¼”ç¤ºè¿è¡Œé”™è¯¯: {e}")
            print(f"âŒ æ¼”ç¤ºè¿è¡Œå‡ºé”™: {e}")
            raise


async def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ¼”ç¤ºç³»ç»Ÿ
        demo = AgenticXGraphRAGDemo()
        
        # è¿è¡Œæ¼”ç¤º
        await demo.run_demo()
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())