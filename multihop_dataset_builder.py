#!/usr/bin/env python3
"""
å¤šè·³å¤æ‚æ¨ç†é—®ç­”å¯¹æ•°æ®é›†æ„å»ºå™¨

åŠŸèƒ½ï¼š
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€TXTã€JSONã€CSVç­‰ï¼‰
- é›†æˆæç¤ºè¯ç®¡ç†å™¨ï¼Œæ”¯æŒé¢†åŸŸé€šç”¨é…ç½®
- æ”¯æŒå¤šç§LLMæä¾›å•†ï¼ˆç™¾ç‚¼ã€OpenAIç­‰ï¼‰
- è¾“å‡ºæ ‡å‡†JSONæ ¼å¼çš„å¤šè·³é—®ç­”å¯¹æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•ï¼š
python multihop_dataset_builder.py --data_path ./documents --output ./output.json --llm_provider bailian --llm_model qwen3-max
"""

import os
import sys
import json
import yaml
import asyncio
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from loguru import logger
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
script_dir = Path(__file__).parent
env_path = script_dir / ".env"
load_dotenv(env_path)

# å¯¼å…¥AgenticXç»„ä»¶
from agenticx.knowledge import Document
from agenticx.knowledge.readers import PDFReader, TextReader, JSONReader, CSVReader
from agenticx.llms import LlmFactory
from agenticx.knowledge.graphers.config import LLMConfig

# å¯¼å…¥æœ¬åœ°ç»„ä»¶
from prompt_manager import PromptManager


class MultihopDatasetBuilder:
    """å¤šè·³æ•°æ®é›†æ„å»ºå™¨"""
    
    def __init__(self, config_path: str = "configs.yml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.prompt_manager = PromptManager("prompts")
        self.llm_client = None
        
        # è®¾ç½®æ—¥å¿—
        logger.add(
            "logs/multihop_builder.log",
            rotation="10 MB",
            retention="7 days",
            level="INFO"
        )
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # æ›¿æ¢ç¯å¢ƒå˜é‡
            self._replace_env_vars(config)
            return config
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def _replace_env_vars(self, obj: Any) -> None:
        """é€’å½’æ›¿æ¢é…ç½®ä¸­çš„ç¯å¢ƒå˜é‡"""
        if isinstance(obj, dict):
            for key, value in obj.items():
                if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
                    env_var = value[2:-1]
                    obj[key] = os.getenv(env_var, value)
                elif isinstance(value, (dict, list)):
                    self._replace_env_vars(value)
        elif isinstance(obj, list):
            for item in obj:
                self._replace_env_vars(item)
    
    async def initialize_llm(self, provider: str = None, model: str = None) -> None:
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„å¼ºæ¨¡å‹é…ç½®
        if provider and model:
            llm_config_dict = {
                'provider': provider,
                'model': model,
                'api_key': self.config['llm']['api_key'],
                'base_url': self.config['llm']['base_url'],
                'temperature': 0.1,
                'max_tokens': 128000,
                'timeout': 600,
                'retry_attempts': 3
            }
        else:
            # ä½¿ç”¨å¼ºæ¨¡å‹é…ç½®
            llm_config_dict = self.config['llm']['strong_model']
        
        # æä¾›å•†ç±»å‹æ˜ å°„
        provider_type_mapping = {
            'openai': 'litellm',
            'anthropic': 'litellm', 
            'litellm': 'litellm',
            'bailian': 'bailian',
            'kimi': 'kimi'
        }
        
        llm_type = provider_type_mapping.get(llm_config_dict['provider'], 'litellm')
        
        # åˆ›å»ºLLMé…ç½®å¯¹è±¡
        llm_config = LLMConfig(
            type=llm_type,
            model=llm_config_dict.get('model'),
            api_key=llm_config_dict.get('api_key'),
            base_url=llm_config_dict.get('base_url'),
            provider=llm_config_dict.get('provider'),
            timeout=llm_config_dict.get('timeout'),
            max_retries=llm_config_dict.get('retry_attempts'),
            temperature=llm_config_dict.get('temperature'),
            max_tokens=llm_config_dict.get('max_tokens')
        )
        
        self.llm_client = LlmFactory.create_llm(llm_config)
        logger.info(f"âœ… LLMåˆå§‹åŒ–å®Œæˆ: {llm_config.provider}/{llm_config.model}")
    
    async def load_documents(self, data_path: str, file_types: List[str] = None) -> List[Document]:
        """åŠ è½½æ–‡æ¡£
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
            file_types: æ”¯æŒçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚['pdf', 'txt', 'json']
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        data_path = Path(data_path)
        
        if file_types is None:
            file_types = ['pdf', 'txt', 'md', 'json', 'csv']
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        if data_path.is_file():
            file_paths = [data_path]
        elif data_path.is_dir():
            file_paths = []
            for file_type in file_types:
                file_paths.extend(data_path.glob(f"*.{file_type}"))
                file_paths.extend(data_path.glob(f"**/*.{file_type}"))
        else:
            logger.error(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
            return []
        
        logger.info(f"ğŸ“„ å‘ç° {len(file_paths)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # åŠ è½½æ–‡æ¡£
        reader_config = self.config['knowledge']['readers']
        
        for file_path in file_paths:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–å™¨
                if file_path.suffix.lower() == '.pdf' and reader_config['pdf']['enabled']:
                    pdf_config = reader_config['pdf'].copy()
                    pdf_config.pop('enabled', None)
                    reader = PDFReader(**pdf_config)
                elif file_path.suffix.lower() in ['.txt', '.md'] and reader_config['text']['enabled']:
                    text_config = reader_config['text'].copy()
                    text_config.pop('enabled', None)
                    reader = TextReader(**text_config)
                elif file_path.suffix.lower() == '.json' and reader_config['json']['enabled']:
                    json_config = reader_config['json'].copy()
                    json_config.pop('enabled', None)
                    reader = JSONReader(**json_config)
                elif file_path.suffix.lower() == '.csv' and reader_config['csv']['enabled']:
                    csv_config = reader_config['csv'].copy()
                    csv_config.pop('enabled', None)
                    reader = CSVReader(**csv_config)
                else:
                    logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                    continue
                
                # è¯»å–æ–‡æ¡£
                result = await reader.read(str(file_path))
                
                # å¤„ç†è¿”å›ç»“æœ
                if isinstance(result, list):
                    for doc in result:
                        documents.append(doc)
                        logger.info(f"âœ… åŠ è½½æ–‡æ¡£: {file_path.name} ({len(doc.content)} å­—ç¬¦)")
                else:
                    documents.append(result)
                    logger.info(f"âœ… åŠ è½½æ–‡æ¡£: {file_path.name} ({len(result.content)} å­—ç¬¦)")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
                continue
        
        logger.info(f"ğŸ“š æ€»è®¡åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def prepare_prompt_variables(self, domain_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """å‡†å¤‡æç¤ºè¯å˜é‡
        
        Args:
            domain_config: é¢†åŸŸç‰¹å®šé…ç½®
            
        Returns:
            æç¤ºè¯å˜é‡å­—å…¸
        """
        # åŠ è½½å¤šè·³æ•°æ®é›†ç”Ÿæˆé…ç½®
        prompt_config = self.prompt_manager.load_prompt("multihop_dataset_generation")
        
        # åŸºç¡€å˜é‡
        variables = {
            'sample_nums': prompt_config.get('config', {}).get('sample_nums', 20),
            'min_documents_per_question': prompt_config.get('config', {}).get('min_documents_per_question', 2),
            'max_answer_sentences': prompt_config.get('config', {}).get('max_answer_sentences', 3),
            'target_language': prompt_config.get('config', {}).get('target_language', 'ä¸­æ–‡'),
            'allowed_terms': prompt_config.get('config', {}).get('allowed_terms', 'è‹±æ–‡æœ¯è¯­'),
        }
        
        # é»˜è®¤å˜é‡å€¼
        default_variables = prompt_config.get('variables', {})
        for key, value in default_variables.items():
            variables[key] = value
        
        # åº”ç”¨é¢†åŸŸç‰¹å®šé…ç½®
        if domain_config:
            variables.update(domain_config)
        
        return variables
    
    async def generate_dataset(self, 
                             documents: List[Document], 
                             domain_config: Dict[str, Any] = None,
                             custom_variables: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¤šè·³æ•°æ®é›†
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            domain_config: é¢†åŸŸç‰¹å®šé…ç½®
            custom_variables: è‡ªå®šä¹‰å˜é‡
            
        Returns:
            å¤šè·³é—®ç­”å¯¹åˆ—è¡¨
        """
        if not self.llm_client:
            logger.error("âŒ LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return []
        
        # å‡†å¤‡æ–‡æ¡£å†…å®¹
        documents_text = ""
        for i, doc in enumerate(documents, 1):
            doc_name = getattr(doc.metadata, 'name', f'document_{i}')
            documents_text += f"\n\n=== æ–‡æ¡£ {i}: {doc_name} ===\n{doc.content}"
        
        # å‡†å¤‡æç¤ºè¯å˜é‡
        variables = self.prepare_prompt_variables(domain_config)
        variables['documents'] = documents_text
        
        # åº”ç”¨è‡ªå®šä¹‰å˜é‡
        if custom_variables:
            variables.update(custom_variables)
        
        # æ ¼å¼åŒ–æç¤ºè¯
        prompt = self.prompt_manager.format_prompt(
            "multihop_dataset_generation", 
            "template", 
            **variables
        )
        
        if not prompt:
            logger.error("âŒ æç¤ºè¯æ ¼å¼åŒ–å¤±è´¥")
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆå¤šè·³æ•°æ®é›†ï¼Œç›®æ ‡æ•°é‡: {variables['sample_nums']}")
        logger.debug(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            # è°ƒç”¨LLMç”Ÿæˆæ•°æ®é›†
            response = await self.llm_client.achat(prompt)
            
            # è§£æJSONå“åº”
            response_text = response.strip()
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            if "```json" in response_text:
                start = response_text.find("```json") + 7
                end = response_text.find("```", start)
                if end != -1:
                    response_text = response_text[start:end].strip()
            
            # è§£æJSON
            dataset = json.loads(response_text)
            
            if not isinstance(dataset, list):
                logger.error("âŒ å“åº”æ ¼å¼é”™è¯¯ï¼šæœŸæœ›JSONæ•°ç»„")
                return []
            
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(dataset)} ä¸ªå¤šè·³é—®ç­”å¯¹")
            
            # éªŒè¯æ•°æ®è´¨é‡
            valid_dataset = self._validate_dataset(dataset)
            logger.info(f"ğŸ“Š éªŒè¯é€šè¿‡: {len(valid_dataset)}/{len(dataset)} ä¸ªé—®ç­”å¯¹")
            
            return valid_dataset
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response_text[:500]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _validate_dataset(self, dataset: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯æ•°æ®é›†è´¨é‡"""
        valid_items = []
        
        for i, item in enumerate(dataset):
            try:
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['query', 'from_docs', 'expected_output', 'criteria_clarify']
                if not all(field in item for field in required_fields):
                    logger.warning(f"âš ï¸ ç¬¬{i+1}ä¸ªé—®ç­”å¯¹ç¼ºå°‘å¿…éœ€å­—æ®µ")
                    continue
                
                # æ£€æŸ¥å­—æ®µç±»å‹
                if not isinstance(item['query'], str) or not item['query'].strip():
                    logger.warning(f"âš ï¸ ç¬¬{i+1}ä¸ªé—®ç­”å¯¹çš„queryæ— æ•ˆ")
                    continue
                
                if not isinstance(item['from_docs'], list) or len(item['from_docs']) < 1:
                    logger.warning(f"âš ï¸ ç¬¬{i+1}ä¸ªé—®ç­”å¯¹çš„from_docsæ— æ•ˆ")
                    continue
                
                if not isinstance(item['expected_output'], str) or not item['expected_output'].strip():
                    logger.warning(f"âš ï¸ ç¬¬{i+1}ä¸ªé—®ç­”å¯¹çš„expected_outputæ— æ•ˆ")
                    continue
                
                valid_items.append(item)
                
            except Exception as e:
                logger.warning(f"âš ï¸ ç¬¬{i+1}ä¸ªé—®ç­”å¯¹éªŒè¯å¤±è´¥: {e}")
                continue
        
        return valid_items
    
    async def save_dataset(self, dataset: List[Dict[str, Any]], output_path: str) -> None:
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")
            logger.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡: {len(dataset)} ä¸ªé—®ç­”å¯¹")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®é›†å¤±è´¥: {e}")
    
    async def build_dataset(self, 
                          data_path: str,
                          output_path: str,
                          llm_provider: str = None,
                          llm_model: str = None,
                          domain_config: Dict[str, Any] = None,
                          file_types: List[str] = None) -> None:
        """æ„å»ºå¤šè·³æ•°æ®é›†çš„ä¸»è¦æµç¨‹
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            llm_provider: LLMæä¾›å•†
            llm_model: LLMæ¨¡å‹åç§°
            domain_config: é¢†åŸŸç‰¹å®šé…ç½®
            file_types: æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        """
        try:
            # 1. åˆå§‹åŒ–LLM
            await self.initialize_llm(llm_provider, llm_model)
            
            # 2. åŠ è½½æ–‡æ¡£
            documents = await self.load_documents(data_path, file_types)
            
            if not documents:
                logger.error("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
                return
            
            # 3. ç”Ÿæˆæ•°æ®é›†
            dataset = await self.generate_dataset(documents, domain_config)
            
            if not dataset:
                logger.error("âŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥")
                return
            
            # 4. ä¿å­˜æ•°æ®é›†
            await self.save_dataset(dataset, output_path)
            
            logger.info("ğŸ‰ å¤šè·³æ•°æ®é›†æ„å»ºå®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†æ„å»ºå¤±è´¥: {e}")
    
    async def load_documents(self, data_path: str, file_types: List[str] = None) -> List[Document]:
        """åŠ è½½æ–‡æ¡£ï¼ˆå¤ç”¨main.pyä¸­çš„é€»è¾‘ï¼‰"""
        documents = []
        data_path = Path(data_path)
        
        if file_types is None:
            file_types = ['pdf', 'txt', 'md', 'json', 'csv']
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        if data_path.is_file():
            file_paths = [data_path]
        elif data_path.is_dir():
            file_paths = []
            for file_type in file_types:
                file_paths.extend(data_path.glob(f"*.{file_type}"))
                file_paths.extend(data_path.glob(f"**/*.{file_type}"))
        else:
            logger.error(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
            return []
        
        logger.info(f"ğŸ“„ å‘ç° {len(file_paths)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # åŠ è½½æ–‡æ¡£
        reader_config = self.config['knowledge']['readers']
        
        for file_path in file_paths:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–å™¨
                if file_path.suffix.lower() == '.pdf' and reader_config['pdf']['enabled']:
                    pdf_config = reader_config['pdf'].copy()
                    pdf_config.pop('enabled', None)
                    reader = PDFReader(**pdf_config)
                elif file_path.suffix.lower() in ['.txt', '.md'] and reader_config['text']['enabled']:
                    text_config = reader_config['text'].copy()
                    text_config.pop('enabled', None)
                    reader = TextReader(**text_config)
                elif file_path.suffix.lower() == '.json' and reader_config['json']['enabled']:
                    json_config = reader_config['json'].copy()
                    json_config.pop('enabled', None)
                    reader = JSONReader(**json_config)
                elif file_path.suffix.lower() == '.csv' and reader_config['csv']['enabled']:
                    csv_config = reader_config['csv'].copy()
                    csv_config.pop('enabled', None)
                    reader = CSVReader(**csv_config)
                else:
                    logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                    continue
                
                # è¯»å–æ–‡æ¡£
                result = await reader.read(str(file_path))
                
                # å¤„ç†è¿”å›ç»“æœ
                if isinstance(result, list):
                    for doc in result:
                        documents.append(doc)
                        logger.debug(f"ğŸ“„ åŠ è½½æ–‡æ¡£: {file_path.name}")
                else:
                    documents.append(result)
                    logger.debug(f"ğŸ“„ åŠ è½½æ–‡æ¡£: {file_path.name}")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
                continue
        
        logger.info(f"ğŸ“š æ€»è®¡åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents


def create_domain_config(domain: str) -> Dict[str, Any]:
    """åˆ›å»ºé¢†åŸŸç‰¹å®šé…ç½®
    
    Args:
        domain: é¢†åŸŸåç§° (technology, finance, medical, generalç­‰)
        
    Returns:
        é¢†åŸŸé…ç½®å­—å…¸
    """
    domain_configs = {
        'technology': {
            'domain_guidance': '''
è¯·æ ¹æ®æ–‡æ¡£å†…å®¹è‡ªåŠ¨è¯†åˆ«æŠ€æœ¯ä¸»é¢˜ï¼ˆå¦‚ç®—æ³•ã€æ¡†æ¶ã€ç³»ç»Ÿæ¶æ„ç­‰ï¼‰ï¼Œç„¶åé€‰æ‹©ä¸¤ä¸ªä¸åŒæŠ€æœ¯ä¸»é¢˜è¿›è¡Œç»„åˆï¼Œç”Ÿæˆè·¨æ–‡æ¡£å¤šè·³é—®é¢˜ã€‚
æ³¨æ„è¯†åˆ«æ–‡æ¡£ä¸­çš„æ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µã€å®ç°æ–¹æ³•ã€æ€§èƒ½æŒ‡æ ‡ï¼Œå¹¶å¯»æ‰¾å®ƒä»¬ä¹‹é—´çš„æŠ€æœ¯å…³è”å’Œå¯¹æ¯”ç‚¹ã€‚
            '''.strip(),
            'domain_specific_terms': 'ç®—æ³•åç§°ã€æ€§èƒ½æŒ‡æ ‡ã€æŠ€æœ¯æ¶æ„ã€æ¡†æ¶ç»„ä»¶',
            'comparison_aspect': 'æŠ€æœ¯æ¶æ„/ç®—æ³•è®¾è®¡/æ€§èƒ½è¡¨ç°/å®ç°å¤æ‚åº¦',
            'mechanism_name': 'æ ¸å¿ƒç®—æ³•/æŠ€æœ¯æœºåˆ¶/æ¶æ„è®¾è®¡',
            'target_aspect': 'æ€§èƒ½/æ•ˆç‡/å‡†ç¡®æ€§/å¯æ‰©å±•æ€§',
            'challenge': 'æŠ€æœ¯æŒ‘æˆ˜/æ€§èƒ½ç“¶é¢ˆ/æ‰©å±•æ€§é—®é¢˜'
        },
        'finance': {
            'domain_guidance': '''
è¯·æ ¹æ®æ–‡æ¡£å†…å®¹è‡ªåŠ¨è¯†åˆ«é‡‘èä¸»é¢˜ï¼ˆå¦‚äº¤æ˜“ç­–ç•¥ã€é£é™©ç®¡ç†ã€å¸‚åœºåˆ†æç­‰ï¼‰ï¼Œç„¶åé€‰æ‹©ä¸¤ä¸ªä¸åŒé‡‘èä¸»é¢˜è¿›è¡Œç»„åˆï¼Œç”Ÿæˆè·¨æ–‡æ¡£å¤šè·³é—®é¢˜ã€‚
æ³¨æ„è¯†åˆ«æ–‡æ¡£ä¸­çš„é‡‘èæ¦‚å¿µã€ç­–ç•¥æ–¹æ³•ã€é£é™©æŒ‡æ ‡ï¼Œå¹¶å¯»æ‰¾å®ƒä»¬ä¹‹é—´çš„ä¸šåŠ¡å…³è”å’Œå¯¹æ¯”ç‚¹ã€‚
            '''.strip(),
            'domain_specific_terms': 'äº¤æ˜“ç­–ç•¥ã€é£é™©æŒ‡æ ‡ã€æ”¶ç›Šç‡ã€é‡‘èå·¥å…·',
            'comparison_aspect': 'ç­–ç•¥è®¾è®¡/é£é™©æ§åˆ¶/æ”¶ç›Šè¡¨ç°/å¸‚åœºé€‚åº”æ€§',
            'mechanism_name': 'äº¤æ˜“æœºåˆ¶/é£æ§ç­–ç•¥/åˆ†ææ–¹æ³•',
            'target_aspect': 'æ”¶ç›Šç‡/é£é™©æ§åˆ¶/æµåŠ¨æ€§/ç¨³å®šæ€§',
            'challenge': 'å¸‚åœºæ³¢åŠ¨/é£é™©æ§åˆ¶/ç›‘ç®¡è¦æ±‚'
        },
        'medical': {
            'domain_guidance': '''
è¯·æ ¹æ®æ–‡æ¡£å†…å®¹è‡ªåŠ¨è¯†åˆ«åŒ»å­¦ä¸»é¢˜ï¼ˆå¦‚è¯Šæ–­æ–¹æ³•ã€æ²»ç–—æ–¹æ¡ˆã€è¯ç‰©æœºåˆ¶ç­‰ï¼‰ï¼Œç„¶åé€‰æ‹©ä¸¤ä¸ªä¸åŒåŒ»å­¦ä¸»é¢˜è¿›è¡Œç»„åˆï¼Œç”Ÿæˆè·¨æ–‡æ¡£å¤šè·³é—®é¢˜ã€‚
æ³¨æ„è¯†åˆ«æ–‡æ¡£ä¸­çš„åŒ»å­¦æ¦‚å¿µã€æ²»ç–—æ–¹æ³•ã€ä¸´åºŠæŒ‡æ ‡ï¼Œå¹¶å¯»æ‰¾å®ƒä»¬ä¹‹é—´çš„åŒ»å­¦å…³è”å’Œå¯¹æ¯”ç‚¹ã€‚
            '''.strip(),
            'domain_specific_terms': 'ç–¾ç—…åç§°ã€è¯ç‰©æˆåˆ†ã€æ²»ç–—æ–¹æ¡ˆã€ä¸´åºŠæŒ‡æ ‡',
            'comparison_aspect': 'æ²»ç–—æ•ˆæœ/å‰¯ä½œç”¨/é€‚åº”ç—‡/æ²»ç–—æœºåˆ¶',
            'mechanism_name': 'æ²»ç–—æœºåˆ¶/è¯ç‰©ä½œç”¨/è¯Šæ–­æ–¹æ³•',
            'target_aspect': 'ç–—æ•ˆ/å®‰å…¨æ€§/é€‚ç”¨æ€§/ä¾¿åˆ©æ€§',
            'challenge': 'å¤æ‚ç—…ä¾‹/è¯ç‰©è€å—/ä¸ªä½“å·®å¼‚'
        },
        'general': {
            'domain_guidance': '''
è¯·æ ¹æ®æ–‡æ¡£å†…å®¹è‡ªåŠ¨è¯†åˆ«ä¸»è¦ä¸»é¢˜å’Œæ¦‚å¿µï¼Œç„¶åé€‰æ‹©ä¸¤ä¸ªä¸åŒä¸»é¢˜è¿›è¡Œç»„åˆï¼Œç”Ÿæˆè·¨æ–‡æ¡£å¤šè·³é—®é¢˜ã€‚
æ³¨æ„è¯†åˆ«æ–‡æ¡£ä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€æ–¹æ³•ã€ç³»ç»Ÿæˆ–ç†è®ºï¼Œå¹¶å¯»æ‰¾å®ƒä»¬ä¹‹é—´çš„æ½œåœ¨å…³è”ã€‚
            '''.strip(),
            'domain_specific_terms': 'ä¸“ä¸šæœ¯è¯­ã€å…³é”®æ¦‚å¿µã€æ–¹æ³•åç§°ã€æŒ‡æ ‡æ•°æ®',
            'comparison_aspect': 'è®¾è®¡ç†å¿µ/å®ç°æ–¹æ³•/æ•ˆæœè¡¨ç°/åº”ç”¨åœºæ™¯',
            'mechanism_name': 'æ ¸å¿ƒæœºåˆ¶/ä¸»è¦æ–¹æ³•/å…³é”®ç­–ç•¥',
            'target_aspect': 'æ•ˆæœ/æ•ˆç‡/å‡†ç¡®æ€§/é€‚ç”¨æ€§',
            'challenge': 'å¤æ‚åœºæ™¯/æ€§èƒ½è¦æ±‚/å®é™…é™åˆ¶'
        }
    }
    
    return domain_configs.get(domain, domain_configs['general'])


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šè·³å¤æ‚æ¨ç†é—®ç­”å¯¹æ•°æ®é›†æ„å»ºå™¨")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--data_path", required=True, help="æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰")
    parser.add_argument("--output", required=True, help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    
    # LLMé…ç½®å‚æ•°
    parser.add_argument("--llm_provider", help="LLMæä¾›å•† (bailian, openai, anthropicç­‰)")
    parser.add_argument("--llm_model", help="LLMæ¨¡å‹åç§°")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--config", default="configs.yml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--domain", default="general", 
                       choices=['technology', 'finance', 'medical', 'general'],
                       help="é¢†åŸŸç±»å‹")
    parser.add_argument("--file_types", nargs='+', 
                       default=['pdf', 'txt', 'md', 'json', 'csv'],
                       help="æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
    parser.add_argument("--sample_nums", type=int, help="ç”Ÿæˆé—®é¢˜æ•°é‡")
    parser.add_argument("--min_docs", type=int, help="æ¯ä¸ªé—®é¢˜æœ€å°‘æ¶‰åŠæ–‡æ¡£æ•°")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ„å»ºå™¨
        builder = MultihopDatasetBuilder(args.config)
        
        # å‡†å¤‡é¢†åŸŸé…ç½®
        domain_config = create_domain_config(args.domain)
        
        # å‡†å¤‡è‡ªå®šä¹‰å˜é‡
        custom_variables = {}
        if args.sample_nums:
            custom_variables['sample_nums'] = args.sample_nums
        if args.min_docs:
            custom_variables['min_documents_per_question'] = args.min_docs
        
        # æ„å»ºæ•°æ®é›†
        await builder.build_dataset(
            data_path=args.data_path,
            output_path=args.output,
            llm_provider=args.llm_provider,
            llm_model=args.llm_model,
            domain_config=domain_config,
            custom_variables=custom_variables if custom_variables else None
        )
        
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())